"""
–ü—Ä–∏–º–µ—Ä—ã: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

–≠—Ç–æ—Ç —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pandas, numpy, scikit-learn –∏ –¥—Ä—É–≥–∏—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.datasets import make_classification, make_regression, load_iris, load_boston
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import warnings
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
import sqlite3
from pathlib import Path

warnings.filterwarnings('ignore')

# =============================================================================
# –ü—Ä–∏–º–µ—Ä 1: –ë–∞–∑–æ–≤–∞—è —Ä–∞–±–æ—Ç–∞ —Å pandas DataFrame
# =============================================================================

def pandas_basics_demo():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —Å pandas"""
    
    print("=== Pandas Basics Demo ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    data = {
        'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],
        'Age': [25, 30, 35, 28, 32, 27],
        'City': ['New York', 'London', 'Paris', 'Tokyo', 'Berlin', 'Sydney'],
        'Salary': [50000, 60000, 75000, 55000, 68000, 52000],
        'Department': ['IT', 'HR', 'Finance', 'IT', 'Finance', 'HR'],
        'Join_Date': ['2020-01-15', '2019-06-20', '2018-03-10', '2021-09-05', '2020-11-30', '2022-02-14']
    }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame
    df = pd.DataFrame(data)
    df['Join_Date'] = pd.to_datetime(df['Join_Date'])
    
    print("–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
    print(df)
    print(f"\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ DataFrame:")
    print(df.info())
    
    # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n–û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(df.describe())
    
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\n–°—Ä–µ–¥–Ω—è—è –∑–∞—Ä–ø–ª–∞—Ç–∞ –ø–æ –æ—Ç–¥–µ–ª–∞–º:")
    dept_salary = df.groupby('Department')['Salary'].agg(['mean', 'median', 'count'])
    print(dept_salary)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    high_earners = df[df['Salary'] > 60000]
    print(f"\n–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ —Å –∑–∞—Ä–ø–ª–∞—Ç–æ–π > 60000:")
    print(high_earners[['Name', 'Salary', 'Department']])
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª—è–µ–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
    df['Years_Employed'] = (datetime.now() - df['Join_Date']).dt.days / 365.25
    df['Salary_Category'] = pd.cut(df['Salary'], 
                                   bins=[0, 55000, 65000, float('inf')], 
                                   labels=['Low', 'Medium', 'High'])
    
    print(f"\n–î–∞–Ω–Ω—ã–µ —Å –Ω–æ–≤—ã–º–∏ —Å—Ç–æ–ª–±—Ü–∞–º–∏:")
    print(df[['Name', 'Salary', 'Years_Employed', 'Salary_Category']])
    
    return df

def advanced_pandas_operations():
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å pandas"""
    
    print("\n=== Advanced Pandas Operations ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', periods=365, freq='D')
    
    sales_data = pd.DataFrame({
        'Date': dates,
        'Product': np.random.choice(['A', 'B', 'C'], 365),
        'Region': np.random.choice(['North', 'South', 'East', 'West'], 365),
        'Sales': np.random.normal(1000, 200, 365),
        'Units': np.random.poisson(50, 365),
        'Temperature': np.random.normal(20, 10, 365)
    })
    
    # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    sales_data['Sales'] = np.abs(sales_data['Sales'])
    sales_data['Units'] = np.abs(sales_data['Units'])
    
    print("–î–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥–∞–∂–∞—Ö:")
    print(sales_data.head())
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã
    sales_data.set_index('Date', inplace=True)
    monthly_sales = sales_data.resample('M')['Sales'].agg(['sum', 'mean', 'count'])
    print(f"\n–ú–µ—Å—è—á–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏:")
    print(monthly_sales.head())
    
    # Pivot table
    pivot_table = sales_data.pivot_table(
        values='Sales', 
        index='Product', 
        columns='Region', 
        aggfunc='mean'
    )
    print(f"\nPivot table (—Å—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ–¥–∞–∂–∏ –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º –∏ —Ä–µ–≥–∏–æ–Ω–∞–º):")
    print(pivot_table)
    
    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    correlation_matrix = sales_data[['Sales', 'Units', 'Temperature']].corr()
    print(f"\n–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞:")
    print(correlation_matrix)
    
    # Rolling window analysis
    sales_data['Sales_MA_7'] = sales_data['Sales'].rolling(window=7).mean()
    sales_data['Sales_MA_30'] = sales_data['Sales'].rolling(window=30).mean()
    
    print(f"\n–î–∞–Ω–Ω—ã–µ —Å–æ —Å–∫–æ–ª—å–∑—è—â–∏–º–∏ —Å—Ä–µ–¥–Ω–∏–º–∏:")
    print(sales_data[['Sales', 'Sales_MA_7', 'Sales_MA_30']].head(10))
    
    return sales_data

# =============================================================================
# –ü—Ä–∏–º–µ—Ä 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
# =============================================================================

def data_cleaning_demo():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    print("\n=== Data Cleaning Demo ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ "–≥—Ä—è–∑–Ω—ã—Ö" –¥–∞–Ω–Ω—ã—Ö
    dirty_data = pd.DataFrame({
        'ID': [1, 2, 3, 4, 5, 6, 7, 8],
        'Name': ['Alice Smith', 'bob jones', 'CHARLIE BROWN', 'diana prince', None, 'eve adams', 'frank castle', ''],
        'Email': ['alice@email.com', 'BOB@EMAIL.COM', 'charlie@invalid', None, 'eve@email.com', 'invalid-email', 'frank@email.com', 'test@email.com'],
        'Age': [25, 30, None, 28, 150, 32, -5, 27],
        'Salary': [50000, '60000', 75000, None, 68000, '52k', 'unknown', 45000],
        'Join_Date': ['2020-01-15', '2019/06/20', '2018-03-10', None, '30-11-2020', '2022-02-14', 'invalid', '2021-09-05']
    })
    
    print("–ò—Å—Ö–æ–¥–Ω—ã–µ '–≥—Ä—è–∑–Ω—ã–µ' –¥–∞–Ω–Ω—ã–µ:")
    print(dirty_data)
    print(f"\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö:")
    print(dirty_data.isnull().sum())
    
    # –ö–æ–ø–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    clean_data = dirty_data.copy()
    
    # –û—á–∏—Å—Ç–∫–∞ –∏–º–µ–Ω
    def clean_name(name):
        if pd.isna(name) or name == '':
            return None
        return ' '.join(word.capitalize() for word in str(name).split())
    
    clean_data['Name'] = clean_data['Name'].apply(clean_name)
    
    # –û—á–∏—Å—Ç–∫–∞ email
    def clean_email(email):
        if pd.isna(email):
            return None
        email = str(email).lower()
        if '@' in email and '.' in email:
            return email
        return None
    
    clean_data['Email'] = clean_data['Email'].apply(clean_email)
    
    # –û—á–∏—Å—Ç–∫–∞ –≤–æ–∑—Ä–∞—Å—Ç–∞
    def clean_age(age):
        if pd.isna(age):
            return None
        try:
            age = int(age)
            if 18 <= age <= 100:
                return age
        except:
            pass
        return None
    
    clean_data['Age'] = clean_data['Age'].apply(clean_age)
    
    # –û—á–∏—Å—Ç–∫–∞ –∑–∞—Ä–ø–ª–∞—Ç—ã
    def clean_salary(salary):
        if pd.isna(salary):
            return None
        
        salary_str = str(salary).lower().replace(',', '').replace('k', '000')
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–∞
            import re
            numbers = re.findall(r'\d+', salary_str)
            if numbers:
                return int(numbers[0])
        except:
            pass
        return None
    
    clean_data['Salary'] = clean_data['Salary'].apply(clean_salary)
    
    # –û—á–∏—Å—Ç–∫–∞ –¥–∞—Ç
    def clean_date(date):
        if pd.isna(date):
            return None
        
        date_str = str(date)
        
        # –†–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
        formats = ['%Y-%m-%d', '%Y/%m/%d', '%d-%m-%Y']
        
        for fmt in formats:
            try:
                return pd.to_datetime(date_str, format=fmt)
            except:
                continue
        
        try:
            return pd.to_datetime(date_str)
        except:
            return None
    
    clean_data['Join_Date'] = clean_data['Join_Date'].apply(clean_date)
    
    print(f"\n–û—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
    print(clean_data)
    print(f"\n–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏:")
    print(clean_data.isnull().sum())
    
    # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    # –í–æ–∑—Ä–∞—Å—Ç - –º–µ–¥–∏–∞–Ω–æ–π
    clean_data['Age'].fillna(clean_data['Age'].median(), inplace=True)
    
    # –ó–∞—Ä–ø–ª–∞—Ç–∞ - —Å—Ä–µ–¥–Ω–∏–º –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–π –≥—Ä—É–ø–ø–µ
    age_groups = pd.cut(clean_data['Age'], bins=[0, 30, 40, 100], labels=['Young', 'Middle', 'Senior'])
    clean_data['Age_Group'] = age_groups
    
    for group in ['Young', 'Middle', 'Senior']:
        group_mean_salary = clean_data[clean_data['Age_Group'] == group]['Salary'].mean()
        clean_data.loc[(clean_data['Age_Group'] == group) & (clean_data['Salary'].isna()), 'Salary'] = group_mean_salary
    
    print(f"\n–î–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤:")
    print(clean_data)
    
    return clean_data

# =============================================================================
# –ü—Ä–∏–º–µ—Ä 3: –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (EDA)
# =============================================================================

def exploratory_data_analysis():
    """–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
    
    print("\n=== Exploratory Data Analysis ===")
    
    # –ó–∞–≥—Ä—É–∑–∏–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç iris
    iris = load_iris()
    iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
    iris_df['species'] = iris.target
    iris_df['species_name'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})
    
    print("–î–∞—Ç–∞—Å–µ—Ç Iris:")
    print(iris_df.head())
    
    # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n–û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(iris_df.describe())
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –∫–ª–∞—Å—Å–∞–º
    print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≤–∏–¥–∞–º:")
    print(iris_df['species_name'].value_counts())
    
    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    correlation_matrix = iris_df.select_dtypes(include=[np.number]).corr()
    print(f"\n–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞:")
    print(correlation_matrix)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≥—Ä—É–ø–ø–∞–º
    print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤–∏–¥–∞–º:")
    species_stats = iris_df.groupby('species_name').agg({
        'sepal length (cm)': ['mean', 'std'],
        'sepal width (cm)': ['mean', 'std'],
        'petal length (cm)': ['mean', 'std'],
        'petal width (cm)': ['mean', 'std']
    })
    print(species_stats)
    
    # –í—ã–±—Ä–æ—Å—ã (outliers)
    def detect_outliers(df, column):
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
        return outliers
    
    print(f"\n–í—ã–±—Ä–æ—Å—ã –≤ sepal length:")
    outliers = detect_outliers(iris_df, 'sepal length (cm)')
    print(f"–ù–∞–π–¥–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤: {len(outliers)}")
    if len(outliers) > 0:
        print(outliers[['sepal length (cm)', 'species_name']])
    
    return iris_df

# =============================================================================
# –ü—Ä–∏–º–µ—Ä 4: –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
# =============================================================================

def machine_learning_classification():
    """–ü—Ä–∏–º–µ—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º"""
    
    print("\n=== Machine Learning Classification ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    X, y = make_classification(
        n_samples=1000, 
        n_features=20, 
        n_informative=10, 
        n_redundant=5, 
        n_clusters_per_class=1, 
        random_state=42
    )
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ DataFrame –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
    feature_names = [f'feature_{i}' for i in range(X.shape[1])]
    df = pd.DataFrame(X, columns=feature_names)
    df['target'] = y
    
    print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    print(df['target'].value_counts())
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print(f"\n–†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫:")
    print(f"–û–±—É—á–∞—é—â–∞—è: {X_train.shape}")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è: {X_test.shape}")
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –û–±—É—á–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    models = {
        'Logistic Regression': LogisticRegression(random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\n--- {name} ---")
        
        # –û–±—É—á–µ–Ω–∏–µ
        if name == 'Logistic Regression':
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
        else:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        accuracy = accuracy_score(y_test, y_pred)
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        if name == 'Logistic Regression':
            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
        else:
            cv_scores = cross_val_score(model, X_train, y_train, cv=5)
        
        print(f"–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è (5-fold): {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
        
        # –û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        print(f"–û—Ç—á–µ—Ç –æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
        print(classification_report(y_test, y_pred))
        
        results[name] = {
            'accuracy': accuracy,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    print(f"\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π ===")
    comparison_df = pd.DataFrame(results).T
    print(comparison_df)
    
    return models, results

# =============================================================================
# –ü—Ä–∏–º–µ—Ä 5: –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —Ä–µ–≥—Ä–µ—Å—Å–∏—è
# =============================================================================

def machine_learning_regression():
    """–ü—Ä–∏–º–µ—Ä —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å –º–∞—à–∏–Ω–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º"""
    
    print("\n=== Machine Learning Regression ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    X, y = make_regression(
        n_samples=1000, 
        n_features=10, 
        noise=0.1, 
        random_state=42
    )
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ DataFrame
    feature_names = [f'feature_{i}' for i in range(X.shape[1])]
    df = pd.DataFrame(X, columns=feature_names)
    df['target'] = y
    
    print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:")
    print(df['target'].describe())
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    models = {
        'Linear Regression': LinearRegression(),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\n--- {name} ---")
        
        # –û–±—É—á–µ–Ω–∏–µ
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)
        
        print(f"MSE: {mse:.3f}")
        print(f"RMSE: {rmse:.3f}")
        print(f"R¬≤: {r2:.3f}")
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
        print(f"–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è R¬≤ (5-fold): {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
        
        results[name] = {
            'mse': mse,
            'rmse': rmse,
            'r2': r2,
            'cv_r2_mean': cv_scores.mean(),
            'cv_r2_std': cv_scores.std()
        }
        
        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è Random Forest)
        if hasattr(model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'feature': feature_names,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print(f"–¢–æ–ø-5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            print(feature_importance.head())
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    print(f"\n=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π ===")
    comparison_df = pd.DataFrame(results).T
    print(comparison_df)
    
    return models, results

# =============================================================================
# –ü—Ä–∏–º–µ—Ä 6: –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
# =============================================================================

def hyperparameter_optimization():
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å Grid Search"""
    
    print("\n=== Hyperparameter Optimization ===")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    iris = load_iris()
    X, y = iris.data, iris.target
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ—Ç–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è Random Forest
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 5, 7, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    # Grid Search
    rf = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(
        rf, 
        param_grid, 
        cv=5, 
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )
    
    print("–í—ã–ø–æ–ª–Ω—è–µ–º Grid Search...")
    grid_search.fit(X_train, y_train)
    
    print(f"\n–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(grid_search.best_params_)
    
    print(f"\n–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏: {grid_search.best_score_:.3f}")
    
    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    best_model = grid_search.best_estimator_
    test_accuracy = best_model.score(X_test, y_test)
    print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {test_accuracy:.3f}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Grid Search
    results_df = pd.DataFrame(grid_search.cv_results_)
    
    print(f"\n–¢–æ–ø-5 –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
    top_results = results_df.nlargest(5, 'mean_test_score')[
        ['params', 'mean_test_score', 'std_test_score']
    ]
    print(top_results)
    
    return grid_search

# =============================================================================
# –ü—Ä–∏–º–µ—Ä 7: Pipeline –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
# =============================================================================

def ml_pipeline_example():
    """–ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è ML pipeline"""
    
    print("\n=== ML Pipeline Example ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–º–µ—à–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ)
    np.random.seed(42)
    n_samples = 1000
    
    data = pd.DataFrame({
        'age': np.random.randint(18, 65, n_samples),
        'income': np.random.normal(50000, 15000, n_samples),
        'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples),
        'city': np.random.choice(['New York', 'London', 'Paris', 'Tokyo'], n_samples),
        'experience': np.random.randint(0, 30, n_samples)
    })
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    target = (
        (data['age'] * 0.01) + 
        (data['income'] * 0.00001) + 
        (data['experience'] * 0.02) + 
        np.random.normal(0, 0.1, n_samples)
    )
    data['promoted'] = (target > np.median(target)).astype(int)
    
    print("–î–∞–Ω–Ω—ã–µ –¥–ª—è pipeline:")
    print(data.head())
    print(f"\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:")
    print(data.dtypes)
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    numeric_features = ['age', 'income', 'experience']
    categorical_features = ['education', 'city']
    
    X = data[numeric_features + categorical_features]
    y = data['promoted']
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ preprocessing pipeline
    numeric_transformer = StandardScaler()
    categorical_transformer = OneHotEncoder(drop='first', sparse_output=False)
    
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ]
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ pipeline
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
    ])
    
    print(f"\n–ü–∞–π–ø–ª–∞–π–Ω:")
    print(pipeline)
    
    # –û–±—É—á–µ–Ω–∏–µ pipeline
    pipeline.fit(X_train, y_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\n–¢–æ—á–Ω–æ—Å—Ç—å pipeline: {accuracy:.3f}")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ preprocessing
    feature_names = (numeric_features + 
                    list(pipeline.named_steps['preprocessor']
                         .named_transformers_['cat']
                         .get_feature_names_out(categorical_features)))
    
    print(f"\n–ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ—Å–ª–µ preprocessing ({len(feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):")
    print(feature_names[:10])  # –ü–µ—Ä–≤—ã–µ 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    importances = pipeline.named_steps['classifier'].feature_importances_
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    print(f"\n–¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    print(feature_importance_df.head(10))
    
    return pipeline

# =============================================================================
# –ü—Ä–∏–º–µ—Ä 8: –†–∞–±–æ—Ç–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ä—è–¥–∞–º–∏
# =============================================================================

def time_series_analysis():
    """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
    
    print("\n=== Time Series Analysis ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞
    dates = pd.date_range('2020-01-01', periods=365*2, freq='D')
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ç—Ä–µ–Ω–¥–æ–º –∏ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å—é
    trend = np.linspace(100, 200, len(dates))
    seasonal = 10 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)
    noise = np.random.normal(0, 5, len(dates))
    
    ts_data = pd.DataFrame({
        'date': dates,
        'value': trend + seasonal + noise
    })
    ts_data.set_index('date', inplace=True)
    
    print("–í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥:")
    print(ts_data.head())
    
    # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n–û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(ts_data.describe())
    
    # –†–µ—Å—ç–º–ø–ª–∏–Ω–≥
    monthly_data = ts_data.resample('M').agg({
        'value': ['mean', 'min', 'max', 'std']
    })
    monthly_data.columns = ['mean', 'min', 'max', 'std']
    
    print(f"\n–ú–µ—Å—è—á–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è (–ø–µ—Ä–≤—ã–µ 10 –º–µ—Å—è—Ü–µ–≤):")
    print(monthly_data.head(10))
    
    # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
    ts_data['MA_7'] = ts_data['value'].rolling(window=7).mean()
    ts_data['MA_30'] = ts_data['value'].rolling(window=30).mean()
    ts_data['MA_90'] = ts_data['value'].rolling(window=90).mean()
    
    print(f"\n–î–∞–Ω–Ω—ã–µ —Å–æ —Å–∫–æ–ª—å–∑—è—â–∏–º–∏ —Å—Ä–µ–¥–Ω–∏–º–∏:")
    print(ts_data.head(10))
    
    # –†–∞—Å—á–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π
    ts_data['pct_change'] = ts_data['value'].pct_change()
    ts_data['diff'] = ts_data['value'].diff()
    
    # –í—ã—è–≤–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
    def detect_outliers_ts(series, window=30, threshold=3):
        rolling_mean = series.rolling(window=window).mean()
        rolling_std = series.rolling(window=window).std()
        
        z_scores = np.abs((series - rolling_mean) / rolling_std)
        return z_scores > threshold
    
    ts_data['outlier'] = detect_outliers_ts(ts_data['value'])
    outliers_count = ts_data['outlier'].sum()
    
    print(f"\n–í—ã–±—Ä–æ—Å—ã –Ω–∞–π–¥–µ–Ω—ã: {outliers_count}")
    if outliers_count > 0:
        print("–ü–µ—Ä–≤—ã–µ 5 –≤—ã–±—Ä–æ—Å–æ–≤:")
        print(ts_data[ts_data['outlier']].head()[['value', 'MA_30']])
    
    # –î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
    # –í—ã—á–∏—Å–ª—è–µ–º —Ç—Ä–µ–Ω–¥ –∫–∞–∫ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
    ts_data['trend'] = ts_data['value'].rolling(window=365, center=True).mean()
    
    # –î–µtrended –¥–∞–Ω–Ω—ã–µ
    ts_data['detrended'] = ts_data['value'] - ts_data['trend']
    
    # –°–µ–∑–æ–Ω–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
    ts_data['day_of_year'] = ts_data.index.dayofyear
    seasonal_pattern = ts_data.groupby('day_of_year')['detrended'].mean()
    ts_data['seasonal'] = ts_data['day_of_year'].map(seasonal_pattern)
    
    # –û—Å—Ç–∞—Ç–∫–∏
    ts_data['residual'] = ts_data['detrended'] - ts_data['seasonal']
    
    print(f"\n–î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞:")
    print(ts_data[['value', 'trend', 'seasonal', 'residual']].dropna().head())
    
    return ts_data

# =============================================================================
# –ü—Ä–∏–º–µ—Ä 9: A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
# =============================================================================

def ab_testing_analysis():
    """–ê–Ω–∞–ª–∏–∑ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    print("\n=== A/B Testing Analysis ===")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö A/B —Ç–µ—Å—Ç–∞
    np.random.seed(42)
    
    # –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è –≥—Ä—É–ø–ø–∞ A
    n_control = 1000
    conversion_rate_control = 0.12
    control_conversions = np.random.binomial(1, conversion_rate_control, n_control)
    
    # –¢–µ—Å—Ç–æ–≤–∞—è –≥—Ä—É–ø–ø–∞ B
    n_treatment = 1000
    conversion_rate_treatment = 0.14  # –ù–∞ 2% –≤—ã—à–µ
    treatment_conversions = np.random.binomial(1, conversion_rate_treatment, n_treatment)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame
    ab_data = pd.DataFrame({
        'user_id': range(n_control + n_treatment),
        'group': ['A'] * n_control + ['B'] * n_treatment,
        'converted': np.concatenate([control_conversions, treatment_conversions])
    })
    
    print("–î–∞–Ω–Ω—ã–µ A/B —Ç–µ—Å—Ç–∞:")
    print(ab_data.head())
    
    # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    results = ab_data.groupby('group').agg({
        'converted': ['count', 'sum', 'mean']
    })
    results.columns = ['total_users', 'conversions', 'conversion_rate']
    
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≥—Ä—É–ø–ø–∞–º:")
    print(results)
    
    # –†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
    from scipy import stats
    
    # –î–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞
    control_conversions_total = results.loc['A', 'conversions']
    control_total = results.loc['A', 'total_users']
    treatment_conversions_total = results.loc['B', 'conversions']
    treatment_total = results.loc['B', 'total_users']
    
    # Chi-square —Ç–µ—Å—Ç
    contingency_table = np.array([
        [control_conversions_total, control_total - control_conversions_total],
        [treatment_conversions_total, treatment_total - treatment_conversions_total]
    ])
    
    chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)
    
    print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç (Chi-square):")
    print(f"Chi-square —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {chi2:.4f}")
    print(f"P-value: {p_value:.4f}")
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–æ (p < 0.05): {p_value < 0.05}")
    
    # –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Å–∏–∏
    def proportion_confint(count, total, alpha=0.05):
        """–î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏"""
        prop = count / total
        z = stats.norm.ppf(1 - alpha/2)
        se = np.sqrt(prop * (1 - prop) / total)
        return prop - z * se, prop + z * se
    
    control_ci = proportion_confint(control_conversions_total, control_total)
    treatment_ci = proportion_confint(treatment_conversions_total, treatment_total)
    
    print(f"\n–î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã (95%):")
    print(f"–ì—Ä—É–ø–ø–∞ A: {control_ci[0]:.4f} - {control_ci[1]:.4f}")
    print(f"–ì—Ä—É–ø–ø–∞ B: {treatment_ci[0]:.4f} - {treatment_ci[1]:.4f}")
    
    # Relative lift
    control_rate = results.loc['A', 'conversion_rate']
    treatment_rate = results.loc['B', 'conversion_rate']
    relative_lift = (treatment_rate - control_rate) / control_rate * 100
    
    print(f"\n–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ: {relative_lift:.1f}%")
    
    # –†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ (Cohen's h)
    def cohens_h(p1, p2):
        """Cohen's h –¥–ª—è —Ä–∞–∑–Ω–æ—Å—Ç–∏ –ø—Ä–æ–ø–æ—Ä—Ü–∏–π"""
        return 2 * (np.arcsin(np.sqrt(p1)) - np.arcsin(np.sqrt(p2)))
    
    effect_size = cohens_h(treatment_rate, control_rate)
    print(f"–†–∞–∑–º–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∞ (Cohen's h): {effect_size:.4f}")
    
    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ —ç—Ñ—Ñ–µ–∫—Ç–∞
    if abs(effect_size) < 0.2:
        effect_interpretation = "–ú–∞–ª—ã–π"
    elif abs(effect_size) < 0.5:
        effect_interpretation = "–°—Ä–µ–¥–Ω–∏–π"
    else:
        effect_interpretation = "–ë–æ–ª—å—à–æ–π"
    
    print(f"–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ —ç—Ñ—Ñ–µ–∫—Ç–∞: {effect_interpretation}")
    
    return ab_data, results

# =============================================================================
# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
# =============================================================================

def main():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ ML"""
    
    print("=== –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ===\n")
    
    # 1. Pandas basics
    df_basic = pandas_basics_demo()
    
    # 2. Advanced pandas
    sales_df = advanced_pandas_operations()
    
    # 3. Data cleaning
    clean_df = data_cleaning_demo()
    
    # 4. EDA
    iris_df = exploratory_data_analysis()
    
    # 5. Classification ML
    classification_models, classification_results = machine_learning_classification()
    
    # 6. Regression ML
    regression_models, regression_results = machine_learning_regression()
    
    # 7. Hyperparameter optimization
    grid_search_result = hyperparameter_optimization()
    
    # 8. ML Pipeline
    pipeline = ml_pipeline_example()
    
    # 9. Time series analysis
    ts_data = time_series_analysis()
    
    # 10. A/B testing
    ab_data, ab_results = ab_testing_analysis()
    
    print("\n=== –°–≤–æ–¥–∫–∞ ===")
    print("‚úÖ Pandas basics –∏ advanced –æ–ø–µ—Ä–∞—Ü–∏–∏")
    print("‚úÖ –û—á–∏—Å—Ç–∫–∞ –∏ preprocessing –¥–∞–Ω–Ω—ã—Ö")
    print("‚úÖ –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (EDA)")
    print("‚úÖ –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏—è")
    print("‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    print("‚úÖ ML Pipeline —Å preprocessing")
    print("‚úÖ –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤")
    print("‚úÖ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑")
    
    print("\n–í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏! üìäü§ñ")

if __name__ == "__main__":
    main() 