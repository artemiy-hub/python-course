#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è: –†–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–∞–º–∏ –≤ Python

–≠—Ç–æ—Ç —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –¥–ª—è –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π:
- –û—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å —Ñ–∞–π–ª–∞–º–∏
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- –°–∏—Å—Ç–µ–º–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
- –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–æ–≤
- –†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
"""

import os
import json
import csv
import time
import shutil
import tempfile
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Generator
import hashlib


def exercise_01_log_analyzer():
    """
    –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 1: –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥-—Ñ–∞–π–ª–æ–≤
    
    –ó–∞–¥–∞—á–∞:
    –°–æ–∑–¥–∞–π—Ç–µ —Å–∏—Å—Ç–µ–º—É –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥-—Ñ–∞–π–ª–æ–≤ –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏:
    1. –ü–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ª–æ–≥–æ–≤
    2. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏, IP, —Å—Ç–∞—Ç—É—Å-–∫–æ–¥–∞–º
    3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –æ—Ç—á–µ—Ç–æ–≤
    4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ (>100MB)
    5. –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
    """
    print("=== –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 1: –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥-—Ñ–∞–π–ª–æ–≤ ===")
    
    # –ó–ê–î–ê–ß–ê: –°–æ–∑–¥–∞–π—Ç–µ –ø–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥–æ–≤
    
    # –†–ï–®–ï–ù–ò–ï:
    
    import re
    from collections import defaultdict, Counter
    
    class LogEntry:
        """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏ –ª–æ–≥–∞"""
        
        def __init__(self, ip, timestamp, method, url, status, size, user_agent, referer=""):
            self.ip = ip
            self.timestamp = timestamp
            self.method = method
            self.url = url
            self.status = int(status)
            self.size = int(size) if size != '-' else 0
            self.user_agent = user_agent
            self.referer = referer
        
        def __str__(self):
            return f"{self.ip} [{self.timestamp}] {self.method} {self.url} {self.status} {self.size}"
    
    class LogParser:
        """–ü–∞—Ä—Å–µ—Ä –ª–æ–≥-—Ñ–∞–π–ª–æ–≤"""
        
        def __init__(self):
            # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
            self.patterns = {
                'common': re.compile(
                    r'^(\S+) \S+ \S+ \[([^]]+)\] "(\S+) (\S+) \S+" (\d+) (\S+)$'
                ),
                'combined': re.compile(
                    r'^(\S+) \S+ \S+ \[([^]]+)\] "(\S+) (\S+) \S+" (\d+) (\S+) "([^"]*)" "([^"]*)"$'
                ),
                'custom': re.compile(
                    r'^(\S+) - - \[([^]]+)\] "(\S+) ([^"]+)" (\d+) (\S+) "([^"]*)" "([^"]*)"$'
                )
            }
        
        def parse_line(self, line):
            """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –ª–æ–≥–∞"""
            line = line.strip()
            if not line or line.startswith('#'):
                return None
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            for format_name, pattern in self.patterns.items():
                match = pattern.match(line)
                if match:
                    groups = match.groups()
                    
                    if format_name == 'common':
                        ip, timestamp, method, url, status, size = groups
                        return LogEntry(ip, timestamp, method, url, status, size, "")
                    
                    elif format_name in ['combined', 'custom']:
                        ip, timestamp, method, url, status, size, referer, user_agent = groups
                        return LogEntry(ip, timestamp, method, url, status, size, user_agent, referer)
            
            return None
        
        def parse_file(self, filename):
            """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∞–π–ª–∞"""
            try:
                with open(filename, 'r', encoding='utf-8', errors='ignore') as file:
                    for line_num, line in enumerate(file, 1):
                        entry = self.parse_line(line)
                        if entry:
                            yield entry
                        elif line.strip():  # –ù–µ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞, –Ω–æ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞
                            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–æ–∫—É {line_num}: {line[:50]}...")
            except FileNotFoundError:
                print(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filename}")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
    
    class LogAnalyzer:
        """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥–æ–≤"""
        
        def __init__(self):
            self.parser = LogParser()
            self.stats = defaultdict(int)
            self.ip_stats = Counter()
            self.status_stats = Counter()
            self.url_stats = Counter()
            self.hourly_stats = defaultdict(int)
            self.error_entries = []
        
        def analyze_file(self, filename):
            """–ê–Ω–∞–ª–∏–∑ –ª–æ–≥-—Ñ–∞–π–ª–∞"""
            print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª: {filename}")
            
            start_time = time.time()
            processed_lines = 0
            
            for entry in self.parser.parse_file(filename):
                self._process_entry(entry)
                processed_lines += 1
                
                if processed_lines % 10000 == 0:
                    elapsed = time.time() - start_time
                    rate = processed_lines / elapsed
                    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_lines:,} —Å—Ç—Ä–æ–∫ ({rate:.0f} —Å—Ç—Ä–æ–∫/—Å–µ–∫)")
            
            elapsed = time.time() - start_time
            print(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {processed_lines:,} —Å—Ç—Ä–æ–∫ –∑–∞ {elapsed:.2f}—Å")
        
        def _process_entry(self, entry):
            """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏"""
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            self.stats['total_requests'] += 1
            self.stats['total_bytes'] += entry.size
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ IP
            self.ip_stats[entry.ip] += 1
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å-–∫–æ–¥–∞–º
            self.status_stats[entry.status] += 1
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ URL
            self.url_stats[entry.url] += 1
            
            # –ü–æ—á–∞—Å–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            try:
                # –ü–∞—Ä—Å–∏–º timestamp (—Ñ–æ—Ä–º–∞—Ç: dd/mmm/yyyy:HH:MM:SS +0000)
                date_part = entry.timestamp.split(':')[1]  # –ë–µ—Ä–µ–º —á–∞—Å
                self.hourly_stats[date_part] += 1
            except:
                pass
            
            # –°–æ–±–∏—Ä–∞–µ–º –æ—à–∏–±–∫–∏ (4xx, 5xx)
            if entry.status >= 400:
                self.error_entries.append(entry)
                if entry.status >= 500:
                    self.stats['server_errors'] += 1
                elif entry.status >= 400:
                    self.stats['client_errors'] += 1
        
        def generate_report(self):
            """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞"""
            report = {
                'summary': dict(self.stats),
                'top_ips': self.ip_stats.most_common(10),
                'status_codes': dict(self.status_stats),
                'top_urls': self.url_stats.most_common(20),
                'hourly_distribution': dict(self.hourly_stats),
                'error_rate': (self.stats['client_errors'] + self.stats['server_errors']) / max(self.stats['total_requests'], 1) * 100
            }
            
            return report
        
        def print_report(self):
            """–í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞ –Ω–∞ —ç–∫—Ä–∞–Ω"""
            report = self.generate_report()
            
            print("\n" + "="*50)
            print("–û–¢–ß–ï–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –õ–û–ì–û–í")
            print("="*50)
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            print(f"  –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {report['summary']['total_requests']:,}")
            print(f"  –ü–µ—Ä–µ–¥–∞–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {report['summary']['total_bytes']:,} –±–∞–π—Ç")
            print(f"  –ö–ª–∏–µ–Ω—Ç—Å–∫–∏–µ –æ—à–∏–±–∫–∏ (4xx): {report['summary'].get('client_errors', 0):,}")
            print(f"  –°–µ—Ä–≤–µ—Ä–Ω—ã–µ –æ—à–∏–±–∫–∏ (5xx): {report['summary'].get('server_errors', 0):,}")
            print(f"  –ü—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫: {report['error_rate']:.2f}%")
            
            # –¢–æ–ø IP-–∞–¥—Ä–µ—Å–æ–≤
            print(f"\nüåê –¢–û–ü-10 IP –ê–î–†–ï–°–û–í:")
            for ip, count in report['top_ips']:
                percentage = (count / report['summary']['total_requests']) * 100
                print(f"  {ip:15} - {count:6,} –∑–∞–ø—Ä–æ—Å–æ–≤ ({percentage:.1f}%)")
            
            # –°—Ç–∞—Ç—É—Å-–∫–æ–¥—ã
            print(f"\nüìà –°–¢–ê–¢–£–°-–ö–û–î–´:")
            for status, count in sorted(report['status_codes'].items()):
                percentage = (count / report['summary']['total_requests']) * 100
                print(f"  {status}: {count:6,} ({percentage:.1f}%)")
            
            # –¢–æ–ø URL
            print(f"\nüîó –¢–û–ü-10 URL:")
            for url, count in report['top_urls'][:10]:
                percentage = (count / report['summary']['total_requests']) * 100
                print(f"  {count:6,} ({percentage:.1f}%) - {url[:60]}")
        
        def export_report(self, format='json', filename=None):
            """–≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª"""
            report = self.generate_report()
            
            if filename is None:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f"log_report_{timestamp}.{format}"
            
            if format == 'json':
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
            
            elif format == 'csv':
                with open(filename, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    
                    # –¢–æ–ø IP
                    writer.writerow(['Top IPs'])
                    writer.writerow(['IP', 'Requests', 'Percentage'])
                    for ip, count in report['top_ips']:
                        pct = (count / report['summary']['total_requests']) * 100
                        writer.writerow([ip, count, f"{pct:.2f}%"])
                    
                    writer.writerow([])  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
                    
                    # –°—Ç–∞—Ç—É—Å –∫–æ–¥—ã
                    writer.writerow(['Status Codes'])
                    writer.writerow(['Status', 'Count', 'Percentage'])
                    for status, count in sorted(report['status_codes'].items()):
                        pct = (count / report['summary']['total_requests']) * 100
                        writer.writerow([status, count, f"{pct:.2f}%"])
            
            print(f"–û—Ç—á–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ {filename}")
            return filename
    
    print("1. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ª–æ–≥-—Ñ–∞–π–ª–∞:")
    
    def create_test_log_file():
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ª–æ–≥-—Ñ–∞–π–ª–∞"""
        
        log_file = "test_access.log"
        
        # –®–∞–±–ª–æ–Ω—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –ª–æ–≥–æ–≤
        ips = ['192.168.1.10', '10.0.0.5', '203.0.113.15', '198.51.100.25', '172.16.0.8']
        methods = ['GET', 'POST', 'PUT', 'DELETE']
        urls = [
            '/', '/index.html', '/about', '/contact', '/products',
            '/api/users', '/api/orders', '/login', '/logout', '/search',
            '/static/css/style.css', '/static/js/app.js', '/images/logo.png'
        ]
        status_codes = [200, 200, 200, 200, 301, 302, 404, 500, 503]  # –ë–æ–ª—å—à–µ —É—Å–ø–µ—à–Ω—ã—Ö
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        ]
        
        import random
        
        with open(log_file, 'w', encoding='utf-8') as f:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
            base_time = datetime.now() - timedelta(days=1)
            
            for i in range(50000):  # 50k –∑–∞–ø–∏—Å–µ–π
                # –°–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                ip = random.choice(ips)
                timestamp_dt = base_time + timedelta(seconds=random.randint(0, 86400))
                timestamp = timestamp_dt.strftime('%d/%b/%Y:%H:%M:%S +0000')
                method = random.choice(methods)
                url = random.choice(urls)
                status = random.choice(status_codes)
                size = random.randint(100, 50000)
                user_agent = random.choice(user_agents)
                
                # –§–æ—Ä–º–∞—Ç Combined Log Format
                line = f'{ip} - - [{timestamp}] "{method} {url} HTTP/1.1" {status} {size} "-" "{user_agent}"\n'
                f.write(line)
        
        file_size = os.path.getsize(log_file)
        print(f"–°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π –ª–æ–≥-—Ñ–∞–π–ª: {log_file} ({file_size:,} –±–∞–π—Ç)")
        
        return log_file
    
    log_file = create_test_log_file()
    
    print("\n2. –ê–Ω–∞–ª–∏–∑ –ª–æ–≥-—Ñ–∞–π–ª–∞:")
    
    analyzer = LogAnalyzer()
    analyzer.analyze_file(log_file)
    
    print("\n3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞:")
    analyzer.print_report()
    
    print("\n4. –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–æ–≤:")
    json_report = analyzer.export_report('json')
    csv_report = analyzer.export_report('csv')
    
    print("\n5. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º:")
    
    class LogFilter:
        """–§–∏–ª—å—Ç—Ä –¥–ª—è –ª–æ–≥–æ–≤"""
        
        def __init__(self, analyzer):
            self.analyzer = analyzer
        
        def filter_by_status(self, filename, status_codes):
            """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å—Ç–∞—Ç—É—Å-–∫–æ–¥–∞–º"""
            filtered_file = f"filtered_status_{'-'.join(map(str, status_codes))}.log"
            
            count = 0
            with open(filtered_file, 'w', encoding='utf-8') as out_file:
                for entry in self.analyzer.parser.parse_file(filename):
                    if entry.status in status_codes:
                        out_file.write(str(entry) + '\n')
                        count += 1
            
            print(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ —Å—Ç–∞—Ç—É—Å-–∫–æ–¥–∞–º {status_codes}: {count} –∑–∞–ø–∏—Å–µ–π ‚Üí {filtered_file}")
            return filtered_file
        
        def filter_by_ip(self, filename, ip_addresses):
            """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ IP-–∞–¥—Ä–µ—Å–∞–º"""
            filtered_file = f"filtered_ip.log"
            
            count = 0
            with open(filtered_file, 'w', encoding='utf-8') as out_file:
                for entry in self.analyzer.parser.parse_file(filename):
                    if entry.ip in ip_addresses:
                        out_file.write(str(entry) + '\n')
                        count += 1
            
            print(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ IP {ip_addresses}: {count} –∑–∞–ø–∏—Å–µ–π ‚Üí {filtered_file}")
            return filtered_file
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
    log_filter = LogFilter(analyzer)
    
    # –§–∏–ª—å—Ç—Ä –æ—à–∏–±–æ–∫
    error_file = log_filter.filter_by_status(log_file, [404, 500, 503])
    
    # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–æ–ø IP
    top_ips = [ip for ip, count in analyzer.ip_stats.most_common(3)]
    top_ip_file = log_filter.filter_by_ip(log_file, top_ips)
    
    print("\n6. –û—á–∏—Å—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤:")
    
    # –°–ø–∏—Å–æ–∫ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    created_files = [log_file, json_report, csv_report, error_file, top_ip_file]
    
    total_size = 0
    for filename in created_files:
        if os.path.exists(filename):
            size = os.path.getsize(filename)
            total_size += size
            print(f"üìÑ {filename}: {size:,} –±–∞–π—Ç")
    
    print(f"üì¶ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤: {total_size:,} –±–∞–π—Ç")
    
    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
    for filename in created_files:
        if os.path.exists(filename):
            os.remove(filename)
    
    print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã")


def exercise_02_data_converter():
    """
    –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 2: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    
    –ó–∞–¥–∞—á–∞:
    –°–æ–∑–¥–∞–π—Ç–µ —Å–∏—Å—Ç–µ–º—É –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É —Ñ–æ—Ä–º–∞—Ç–∞–º–∏:
    1. –ü–æ–¥–¥–µ—Ä–∂–∫–∞ JSON, CSV, XML, YAML
    2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞
    3. –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
    4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –ø–æ —á–∞—Å—Ç—è–º
    5. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
    """
    print("=== –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 2: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä –¥–∞–Ω–Ω—ã—Ö ===")
    
    # –ó–ê–î–ê–ß–ê: –°–æ–∑–¥–∞–π—Ç–µ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    
    # –†–ï–®–ï–ù–ò–ï:
    
    import xml.etree.ElementTree as ET
    from abc import ABC, abstractmethod
    
    class DataFormat(ABC):
        """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
        
        @abstractmethod
        def can_read(self, filename):
            """–ú–æ–∂–µ—Ç –ª–∏ —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª"""
            pass
        
        @abstractmethod
        def read(self, filename):
            """–ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞"""
            pass
        
        @abstractmethod
        def write(self, data, filename):
            """–ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª"""
            pass
        
        @property
        @abstractmethod
        def extension(self):
            """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"""
            pass
    
    class JSONFormat(DataFormat):
        """–§–æ—Ä–º–∞—Ç JSON"""
        
        def can_read(self, filename):
            return filename.lower().endswith('.json')
        
        def read(self, filename):
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        def write(self, data, filename):
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        
        @property
        def extension(self):
            return '.json'
    
    class CSVFormat(DataFormat):
        """–§–æ—Ä–º–∞—Ç CSV"""
        
        def can_read(self, filename):
            return filename.lower().endswith('.csv')
        
        def read(self, filename):
            data = []
            with open(filename, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    data.append(dict(row))
            return data
        
        def write(self, data, filename):
            if not data:
                return
            
            with open(filename, 'w', newline='', encoding='utf-8') as f:
                if isinstance(data[0], dict):
                    fieldnames = data[0].keys()
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(data)
                else:
                    writer = csv.writer(f)
                    writer.writerows(data)
        
        @property
        def extension(self):
            return '.csv'
    
    class XMLFormat(DataFormat):
        """–§–æ—Ä–º–∞—Ç XML"""
        
        def can_read(self, filename):
            return filename.lower().endswith('.xml')
        
        def read(self, filename):
            tree = ET.parse(filename)
            root = tree.getroot()
            return self._xml_to_dict(root)
        
        def write(self, data, filename):
            root = self._dict_to_xml(data, 'root')
            tree = ET.ElementTree(root)
            tree.write(filename, encoding='utf-8', xml_declaration=True)
        
        def _xml_to_dict(self, element):
            """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ XML —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ —Å–ª–æ–≤–∞—Ä—å"""
            result = {}
            
            # –ê—Ç—Ä–∏–±—É—Ç—ã —ç–ª–µ–º–µ–Ω—Ç–∞
            if element.attrib:
                result['@attributes'] = element.attrib
            
            # –¢–µ–∫—Å—Ç —ç–ª–µ–º–µ–Ω—Ç–∞
            if element.text and element.text.strip():
                if len(element) == 0:
                    return element.text.strip()
                result['#text'] = element.text.strip()
            
            # –î–æ—á–µ—Ä–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            children = {}
            for child in element:
                child_data = self._xml_to_dict(child)
                
                if child.tag in children:
                    if not isinstance(children[child.tag], list):
                        children[child.tag] = [children[child.tag]]
                    children[child.tag].append(child_data)
                else:
                    children[child.tag] = child_data
            
            result.update(children)
            
            # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
            if len(result) == 1 and '#text' in result:
                return result['#text']
            
            return result if result else None
        
        def _dict_to_xml(self, data, root_tag):
            """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –≤ XML —ç–ª–µ–º–µ–Ω—Ç"""
            element = ET.Element(root_tag)
            
            if isinstance(data, dict):
                for key, value in data.items():
                    if key == '@attributes':
                        element.attrib.update(value)
                    elif key == '#text':
                        element.text = str(value)
                    else:
                        if isinstance(value, list):
                            for item in value:
                                sub_elem = self._dict_to_xml(item, key)
                                element.append(sub_elem)
                        else:
                            sub_elem = self._dict_to_xml(value, key)
                            element.append(sub_elem)
            else:
                element.text = str(data)
            
            return element
        
        @property
        def extension(self):
            return '.xml'
    
    class DataConverter:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä –¥–∞–Ω–Ω—ã—Ö"""
        
        def __init__(self):
            self.formats = {
                'json': JSONFormat(),
                'csv': CSVFormat(),
                'xml': XMLFormat()
            }
        
        def detect_format(self, filename):
            """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞"""
            for name, format_handler in self.formats.items():
                if format_handler.can_read(filename):
                    return name, format_handler
            
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {filename}")
        
        def convert(self, input_file, output_format, output_file=None):
            """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–∞–π–ª–∞ –≤ –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç"""
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç
            input_format_name, input_format = self.detect_format(input_file)
            print(f"–í—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç: {input_format_name}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç
            if output_format not in self.formats:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç: {output_format}")
            
            output_format_handler = self.formats[output_format]
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if output_file is None:
                base_name = os.path.splitext(input_file)[0]
                output_file = base_name + output_format_handler.extension
            
            print(f"–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç: {output_format}")
            print(f"–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {output_file}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
            print("–ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
            start_time = time.time()
            
            try:
                data = input_format.read(input_file)
                read_time = time.time() - start_time
                
                print(f"–î–∞–Ω–Ω—ã–µ –ø—Ä–æ—á–∏—Ç–∞–Ω—ã –∑–∞ {read_time:.3f}—Å")
                
                if isinstance(data, list):
                    print(f"–ó–∞–ø–∏—Å–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö: {len(data)}")
                elif isinstance(data, dict):
                    print(f"–ö–ª—é—á–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö: {len(data)}")
                
                print("–ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö...")
                write_start = time.time()
                
                output_format_handler.write(data, output_file)
                write_time = time.time() - write_start
                
                print(f"–î–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∞–Ω—ã –∑–∞ {write_time:.3f}—Å")
                
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤
                input_size = os.path.getsize(input_file)
                output_size = os.path.getsize(output_file)
                
                print(f"–†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {input_size:,} –±–∞–π—Ç")
                print(f"–†–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {output_size:,} –±–∞–π—Ç")
                print(f"–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞: {(output_size/input_size-1)*100:+.1f}%")
                
                return output_file
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {e}")
                raise
        
        def batch_convert(self, input_dir, output_format, output_dir=None):
            """–ü–∞–∫–µ—Ç–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
            
            if output_dir is None:
                output_dir = input_dir + f"_converted_to_{output_format}"
            
            os.makedirs(output_dir, exist_ok=True)
            
            converted_files = []
            
            for filename in os.listdir(input_dir):
                input_file = os.path.join(input_dir, filename)
                
                if os.path.isfile(input_file):
                    try:
                        # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç
                        self.detect_format(input_file)
                        
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                        base_name = os.path.splitext(filename)[0]
                        output_ext = self.formats[output_format].extension
                        output_file = os.path.join(output_dir, base_name + output_ext)
                        
                        print(f"\n–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è: {filename}")
                        self.convert(input_file, output_format, output_file)
                        converted_files.append(output_file)
                        
                    except ValueError:
                        print(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞: {filename}")
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ {filename}: {e}")
            
            print(f"\n–ü–∞–∫–µ—Ç–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(converted_files)} —Ñ–∞–π–ª–æ–≤")
            return converted_files
    
    print("1. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    
    def create_test_data():
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö"""
        
        # –û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö
        test_data = [
            {
                "id": 1,
                "name": "–ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤",
                "email": "ivan@example.com",
                "age": 28,
                "city": "–ú–æ—Å–∫–≤–∞",
                "skills": ["Python", "JavaScript", "SQL"],
                "active": True,
                "salary": 95000
            },
            {
                "id": 2,
                "name": "–ú–∞—Ä–∏—è –°–∏–¥–æ—Ä–æ–≤–∞",
                "email": "maria@example.com",
                "age": 25,
                "city": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥",
                "skills": ["Java", "Spring", "Docker"],
                "active": True,
                "salary": 85000
            },
            {
                "id": 3,
                "name": "–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ö–æ–∑–ª–æ–≤",
                "email": "alex@example.com",
                "age": 32,
                "city": "–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫",
                "skills": ["C++", "Python", "Machine Learning"],
                "active": False,
                "salary": 105000
            }
        ]
        
        # –°–æ–∑–¥–∞–µ–º JSON —Ñ–∞–π–ª
        json_file = "test_data.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, indent=2, ensure_ascii=False)
        
        # –°–æ–∑–¥–∞–µ–º CSV —Ñ–∞–π–ª
        csv_file = "test_data.csv"
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            # –£–ø—Ä–æ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è CSV (–±–µ–∑ —Å–ø–∏—Å–∫–æ–≤)
            csv_data = []
            for item in test_data:
                csv_item = item.copy()
                csv_item['skills'] = '|'.join(item['skills'])
                csv_data.append(csv_item)
            
            fieldnames = csv_data[0].keys()
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(csv_data)
        
        print(f"–°–æ–∑–¥–∞–Ω—ã —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã: {json_file}, {csv_file}")
        return json_file, csv_file, test_data
    
    json_file, csv_file, original_data = create_test_data()
    
    print("\n2. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞:")
    
    converter = DataConverter()
    
    # JSON ‚Üí XML
    print("\n–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è JSON ‚Üí XML:")
    xml_file = converter.convert(json_file, 'xml')
    
    # CSV ‚Üí JSON
    print("\n–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è CSV ‚Üí JSON:")
    json_from_csv = converter.convert(csv_file, 'json', 'converted_from_csv.json')
    
    # XML ‚Üí CSV
    print("\n–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è XML ‚Üí CSV:")
    csv_from_xml = converter.convert(xml_file, 'csv', 'converted_from_xml.csv')
    
    print("\n3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö:")
    
    def verify_conversion(original_file, converted_file):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏"""
        
        try:
            orig_format_name, orig_format = converter.detect_format(original_file)
            conv_format_name, conv_format = converter.detect_format(converted_file)
            
            orig_data = orig_format.read(original_file)
            conv_data = conv_format.read(converted_file)
            
            print(f"–ü—Ä–æ–≤–µ—Ä–∫–∞: {orig_format_name} ‚Üí {conv_format_name}")
            
            if isinstance(orig_data, list) and isinstance(conv_data, list):
                print(f"  –ó–∞–ø–∏—Å–µ–π –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ: {len(orig_data)}")
                print(f"  –ó–∞–ø–∏—Å–µ–π –≤ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º: {len(conv_data)}")
                
                if len(orig_data) == len(conv_data):
                    print("  ‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π —Å–æ–≤–ø–∞–¥–∞–µ—Ç")
                else:
                    print("  ‚ùå –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π
            if (isinstance(orig_data, list) and orig_data and 
                isinstance(conv_data, list) and conv_data):
                
                orig_keys = set(orig_data[0].keys()) if isinstance(orig_data[0], dict) else set()
                conv_keys = set(conv_data[0].keys()) if isinstance(conv_data[0], dict) else set()
                
                if orig_keys and conv_keys:
                    common_keys = orig_keys.intersection(conv_keys)
                    print(f"  –û–±—â–∏—Ö –ø–æ–ª–µ–π: {len(common_keys)} –∏–∑ {len(orig_keys)}")
                    
                    if len(common_keys) >= len(orig_keys) * 0.8:  # 80% –ø–æ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ
                        print("  ‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
                    else:
                        print("  ‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—è –ø–æ—Ç–µ—Ä—è–Ω—ã –ø—Ä–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏")
        
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
    verify_conversion(json_file, xml_file)
    verify_conversion(csv_file, json_from_csv)
    verify_conversion(xml_file, csv_from_xml)
    
    print("\n4. –ü–∞–∫–µ—Ç–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è:")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏
    test_dir = "batch_test_data"
    os.makedirs(test_dir, exist_ok=True)
    
    # –ö–æ–ø–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
    shutil.copy(json_file, os.path.join(test_dir, "users.json"))
    shutil.copy(csv_file, os.path.join(test_dir, "employees.csv"))
    shutil.copy(xml_file, os.path.join(test_dir, "data.xml"))
    
    # –ü–∞–∫–µ—Ç–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ JSON
    converted_files = converter.batch_convert(test_dir, 'json')
    
    print("\n5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏:")
    
    all_files = [json_file, csv_file, xml_file, json_from_csv, csv_from_xml] + converted_files
    
    formats_count = {}
    total_size = 0
    
    for filename in all_files:
        if os.path.exists(filename):
            size = os.path.getsize(filename)
            total_size += size
            
            ext = os.path.splitext(filename)[1].lower()
            formats_count[ext] = formats_count.get(ext, 0) + 1
            
            print(f"üìÑ {filename}: {size:,} –±–∞–π—Ç")
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∞–π–ª–æ–≤:")
    for ext, count in formats_count.items():
        print(f"  {ext}: {count} —Ñ–∞–π–ª–æ–≤")
    print(f"üì¶ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size:,} –±–∞–π—Ç")
    
    print("\n6. –û—á–∏—Å—Ç–∫–∞:")
    
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    cleanup_files = all_files + [test_dir]
    
    for item in cleanup_files:
        try:
            if os.path.isfile(item):
                os.remove(item)
            elif os.path.isdir(item):
                shutil.rmtree(item)
        except:
            pass
    
    print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —É–¥–∞–ª–µ–Ω—ã")


def exercise_03_backup_system():
    """
    –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 3: –°–∏—Å—Ç–µ–º–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è
    
    –ó–∞–¥–∞—á–∞:
    –°–æ–∑–¥–∞–π—Ç–µ —Å–∏—Å—Ç–µ–º—É —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏:
    1. –ü–æ–ª–Ω–æ–µ –∏ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
    2. –°–∂–∞—Ç–∏–µ –∞—Ä—Ö–∏–≤–æ–≤
    3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö (—Ö–µ—à–∏)
    4. –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è
    5. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π
    """
    print("=== –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 3: –°–∏—Å—Ç–µ–º–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è ===")
    
    # –ó–ê–î–ê–ß–ê: –°–æ–∑–¥–∞–π—Ç–µ –ø–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è
    
    # –†–ï–®–ï–ù–ò–ï:
    
    import zipfile
    import tarfile
    from datetime import datetime, timedelta
    
    class BackupSystem:
        """–°–∏—Å—Ç–µ–º–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è"""
        
        def __init__(self, backup_dir="backups"):
            self.backup_dir = Path(backup_dir)
            self.backup_dir.mkdir(exist_ok=True)
            self.metadata_file = self.backup_dir / "backup_metadata.json"
            self.metadata = self._load_metadata()
        
        def _load_metadata(self):
            """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –æ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏—è—Ö"""
            if self.metadata_file.exists():
                try:
                    with open(self.metadata_file, 'r', encoding='utf-8') as f:
                        return json.load(f)
                except:
                    pass
            
            return {
                'backups': [],
                'last_full_backup': None,
                'total_backups': 0
            }
        
        def _save_metadata(self):
            """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, indent=2, ensure_ascii=False)
        
        def _calculate_file_hash(self, filepath):
            """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞"""
            hash_md5 = hashlib.md5()
            
            try:
                with open(filepath, 'rb') as f:
                    for chunk in iter(lambda: f.read(4096), b""):
                        hash_md5.update(chunk)
                return hash_md5.hexdigest()
            except:
                return None
        
        def _scan_directory(self, source_dir):
            """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª–æ–≤"""
            file_index = {}
            
            for root, dirs, files in os.walk(source_dir):
                for file in files:
                    filepath = os.path.join(root, file)
                    relative_path = os.path.relpath(filepath, source_dir)
                    
                    try:
                        stat = os.stat(filepath)
                        file_hash = self._calculate_file_hash(filepath)
                        
                        file_index[relative_path] = {
                            'size': stat.st_size,
                            'mtime': stat.st_mtime,
                            'hash': file_hash,
                            'full_path': filepath
                        }
                    except OSError:
                        continue
            
            return file_index
        
        def create_full_backup(self, source_dir, backup_name=None):
            """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏"""
            
            if backup_name is None:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                backup_name = f"full_backup_{timestamp}"
            
            print(f"–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {backup_name}")
            
            # –°–∫–∞–Ω–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            print("–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
            file_index = self._scan_directory(source_dir)
            
            total_files = len(file_index)
            total_size = sum(info['size'] for info in file_index.values())
            
            print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {total_files:,}")
            print(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size:,} –±–∞–π—Ç")
            
            # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ö–∏–≤
            backup_file = self.backup_dir / f"{backup_name}.tar.gz"
            
            with tarfile.open(backup_file, 'w:gz') as tar:
                for relative_path, info in file_index.items():
                    try:
                        tar.add(info['full_path'], arcname=relative_path)
                    except OSError as e:
                        print(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {relative_path}: {e}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            backup_info = {
                'name': backup_name,
                'type': 'full',
                'timestamp': datetime.now().isoformat(),
                'source_dir': str(source_dir),
                'backup_file': str(backup_file),
                'files_count': total_files,
                'total_size': total_size,
                'file_index': file_index
            }
            
            self.metadata['backups'].append(backup_info)
            self.metadata['last_full_backup'] = backup_name
            self.metadata['total_backups'] += 1
            self._save_metadata()
            
            backup_size = backup_file.stat().st_size
            compression_ratio = (1 - backup_size / total_size) * 100
            
            print(f"–†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ–∑–¥–∞–Ω–∞: {backup_file}")
            print(f"–†–∞–∑–º–µ—Ä –∞—Ä—Ö–∏–≤–∞: {backup_size:,} –±–∞–π—Ç")
            print(f"–°—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è: {compression_ratio:.1f}%")
            
            return backup_name
        
        def create_incremental_backup(self, source_dir, backup_name=None):
            """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏"""
            
            if not self.metadata['last_full_backup']:
                print("–ù–µ—Ç –ø–æ–ª–Ω–æ–π —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏. –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—É—é –∫–æ–ø–∏—é...")
                return self.create_full_backup(source_dir, backup_name)
            
            if backup_name is None:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                backup_name = f"incremental_backup_{timestamp}"
            
            print(f"–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {backup_name}")
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
            last_backup = None
            for backup in reversed(self.metadata['backups']):
                if backup['source_dir'] == str(source_dir):
                    last_backup = backup
                    break
            
            if not last_backup:
                print("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∞—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è. –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—É—é...")
                return self.create_full_backup(source_dir, backup_name)
            
            # –°–∫–∞–Ω–∏—Ä—É–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            print("–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...")
            current_index = self._scan_directory(source_dir)
            last_index = last_backup['file_index']
            
            # –ù–∞—Ö–æ–¥–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
            new_files = []
            modified_files = []
            deleted_files = []
            
            # –ù–æ–≤—ã–µ –∏ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            for path, info in current_index.items():
                if path not in last_index:
                    new_files.append(path)
                elif (info['hash'] != last_index[path]['hash'] or 
                      info['mtime'] != last_index[path]['mtime']):
                    modified_files.append(path)
            
            # –£–¥–∞–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            for path in last_index:
                if path not in current_index:
                    deleted_files.append(path)
            
            changed_files = new_files + modified_files
            
            print(f"–ù–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(new_files)}")
            print(f"–ò–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(modified_files)}")
            print(f"–£–¥–∞–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(deleted_files)}")
            
            if not changed_files and not deleted_files:
                print("–ò–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ. –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è –Ω–µ —Å–æ–∑–¥–∞–Ω–∞.")
                return None
            
            # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ö–∏–≤ —Ç–æ–ª—å–∫–æ —Å –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏
            backup_file = self.backup_dir / f"{backup_name}.tar.gz"
            
            with tarfile.open(backup_file, 'w:gz') as tar:
                for relative_path in changed_files:
                    info = current_index[relative_path]
                    try:
                        tar.add(info['full_path'], arcname=relative_path)
                    except OSError as e:
                        print(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {relative_path}: {e}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            backup_info = {
                'name': backup_name,
                'type': 'incremental',
                'timestamp': datetime.now().isoformat(),
                'source_dir': str(source_dir),
                'backup_file': str(backup_file),
                'base_backup': last_backup['name'],
                'new_files': new_files,
                'modified_files': modified_files,
                'deleted_files': deleted_files,
                'files_count': len(changed_files),
                'total_size': sum(current_index[f]['size'] for f in changed_files),
                'file_index': {f: current_index[f] for f in changed_files}
            }
            
            self.metadata['backups'].append(backup_info)
            self.metadata['total_backups'] += 1
            self._save_metadata()
            
            backup_size = backup_file.stat().st_size
            
            print(f"–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∫–æ–ø–∏—è —Å–æ–∑–¥–∞–Ω–∞: {backup_file}")
            print(f"–†–∞–∑–º–µ—Ä –∞—Ä—Ö–∏–≤–∞: {backup_size:,} –±–∞–π—Ç")
            
            return backup_name
        
        def list_backups(self):
            """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π"""
            
            print(f"–í—Å–µ–≥–æ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π: {len(self.metadata['backups'])}")
            print("-" * 80)
            
            for backup in self.metadata['backups']:
                timestamp = datetime.fromisoformat(backup['timestamp'])
                
                print(f"–ò–º—è: {backup['name']}")
                print(f"–¢–∏–ø: {backup['type']}")
                print(f"–î–∞—Ç–∞: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
                print(f"–ò—Å—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {backup['source_dir']}")
                print(f"–§–∞–π–ª–æ–≤: {backup['files_count']:,}")
                print(f"–†–∞–∑–º–µ—Ä: {backup['total_size']:,} –±–∞–π—Ç")
                
                if backup['type'] == 'incremental':
                    print(f"–ë–∞–∑–æ–≤–∞—è –∫–æ–ø–∏—è: {backup['base_backup']}")
                
                print("-" * 80)
        
        def restore_backup(self, backup_name, restore_dir):
            """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏"""
            
            print(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {backup_name}")
            
            # –ù–∞—Ö–æ–¥–∏–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
            backup_info = None
            for backup in self.metadata['backups']:
                if backup['name'] == backup_name:
                    backup_info = backup
                    break
            
            if not backup_info:
                print(f"–†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è {backup_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
            
            restore_path = Path(restore_dir)
            restore_path.mkdir(parents=True, exist_ok=True)
            
            if backup_info['type'] == 'full':
                # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –∫–æ–ø–∏–∏
                backup_file = Path(backup_info['backup_file'])
                
                if not backup_file.exists():
                    print(f"–§–∞–π–ª —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {backup_file}")
                    return False
                
                print("–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏...")
                
                with tarfile.open(backup_file, 'r:gz') as tar:
                    tar.extractall(restore_path)
                
                print(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {backup_info['files_count']} —Ñ–∞–π–ª–æ–≤")
            
            else:
                # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∫–æ–ø–∏–∏
                print("–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∫–æ–ø–∏–∏ —Ç—Ä–µ–±—É–µ—Ç –±–∞–∑–æ–≤—É—é –∫–æ–ø–∏—é...")
                
                # –ù–∞—Ö–æ–¥–∏–º —Ü–µ–ø–æ—á–∫—É —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π
                backup_chain = self._build_backup_chain(backup_name)
                
                if not backup_chain:
                    print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Ü–µ–ø–æ—á–∫—É —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π")
                    return False
                
                print(f"–¶–µ–ø–æ—á–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {' ‚Üí '.join(backup_chain)}")
                
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤ –ø–æ—Ä—è–¥–∫–µ –æ—Ç –ø–æ–ª–Ω–æ–π –∫–æ–ø–∏–∏ –∫ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π
                for chain_backup_name in backup_chain:
                    chain_backup = next(b for b in self.metadata['backups'] 
                                      if b['name'] == chain_backup_name)
                    
                    backup_file = Path(chain_backup['backup_file'])
                    
                    if not backup_file.exists():
                        print(f"–§–∞–π–ª —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {backup_file}")
                        return False
                    
                    print(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ: {chain_backup_name}")
                    
                    with tarfile.open(backup_file, 'r:gz') as tar:
                        tar.extractall(restore_path)
            
            print(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –≤: {restore_path}")
            return True
        
        def _build_backup_chain(self, backup_name):
            """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ü–µ–ø–æ—á–∫–∏ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π"""
            
            chain = []
            current_backup_name = backup_name
            
            while current_backup_name:
                backup = next((b for b in self.metadata['backups'] 
                             if b['name'] == current_backup_name), None)
                
                if not backup:
                    break
                
                chain.insert(0, current_backup_name)
                
                if backup['type'] == 'full':
                    break
                
                current_backup_name = backup.get('base_backup')
            
            return chain
        
        def verify_backup(self, backup_name):
            """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏"""
            
            print(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏: {backup_name}")
            
            backup_info = next((b for b in self.metadata['backups'] 
                              if b['name'] == backup_name), None)
            
            if not backup_info:
                print(f"–†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è {backup_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
            
            backup_file = Path(backup_info['backup_file'])
            
            if not backup_file.exists():
                print(f"–§–∞–π–ª —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {backup_file}")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä—Ö–∏–≤
            try:
                with tarfile.open(backup_file, 'r:gz') as tar:
                    members = tar.getmembers()
                
                print(f"–ê—Ä—Ö–∏–≤ —Å–æ–¥–µ—Ä–∂–∏—Ç {len(members)} —Ñ–∞–π–ª–æ–≤")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
                expected_count = backup_info['files_count']
                
                if len(members) == expected_count:
                    print("‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º")
                else:
                    print(f"‚ùå –û–∂–∏–¥–∞–ª–æ—Å—å {expected_count} —Ñ–∞–π–ª–æ–≤, –Ω–∞–π–¥–µ–Ω–æ {len(members)}")
                    return False
                
                print("‚úÖ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è –ø—Ä–æ—à–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫—É —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏")
                return True
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞—Ä—Ö–∏–≤–∞: {e}")
                return False
        
        def cleanup_old_backups(self, keep_days=30):
            """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π"""
            
            cutoff_date = datetime.now() - timedelta(days=keep_days)
            
            removed_backups = []
            
            for backup in self.metadata['backups'][:]:  # –ö–æ–ø–∏—è —Å–ø–∏—Å–∫–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è
                backup_date = datetime.fromisoformat(backup['timestamp'])
                
                if backup_date < cutoff_date:
                    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏
                    backup_file = Path(backup['backup_file'])
                    
                    if backup_file.exists():
                        backup_file.unlink()
                    
                    # –£–¥–∞–ª—è–µ–º –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                    self.metadata['backups'].remove(backup)
                    removed_backups.append(backup['name'])
            
            if removed_backups:
                self._save_metadata()
                print(f"–£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π: {len(removed_backups)}")
                for name in removed_backups:
                    print(f"  - {name}")
            else:
                print("–°—Ç–∞—Ä—ã—Ö —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    print("1. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ñ–∞–π–ª–æ–≤:")
    
    def create_test_structure():
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è"""
        
        test_dir = Path("test_source")
        test_dir.mkdir(exist_ok=True)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª—ã –∏ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        (test_dir / "documents").mkdir(exist_ok=True)
        (test_dir / "images").mkdir(exist_ok=True)
        (test_dir / "projects").mkdir(exist_ok=True)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
        files_to_create = [
            ("readme.txt", "–≠—Ç–æ –≥–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–æ–µ–∫—Ç–∞\n" * 100),
            ("config.json", json.dumps({"setting1": "value1", "setting2": 42}, indent=2)),
            ("documents/letter.txt", "–í–∞–∂–Ω–æ–µ –ø–∏—Å—å–º–æ\n" * 50),
            ("documents/report.csv", "name,value\nItem1,100\nItem2,200\n"),
            ("images/placeholder.txt", "–§–∞–π–ª-–∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"),
            ("projects/main.py", "#!/usr/bin/env python3\nprint('Hello, World!')\n" * 20),
        ]
        
        total_size = 0
        for filepath, content in files_to_create:
            full_path = test_dir / filepath
            full_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(full_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            total_size += len(content.encode('utf-8'))
        
        print(f"–°–æ–∑–¥–∞–Ω–∞ —Ç–µ—Å—Ç–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ {test_dir}")
        print(f"–§–∞–π–ª–æ–≤: {len(files_to_create)}")
        print(f"–û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size:,} –±–∞–π—Ç")
        
        return test_dir
    
    test_source = create_test_structure()
    
    print("\n2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è:")
    
    backup_system = BackupSystem("test_backups")
    
    print("\n3. –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–π —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏:")
    
    full_backup_name = backup_system.create_full_backup(test_source)
    
    print("\n4. –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤:")
    
    # –ò–∑–º–µ–Ω—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∞–π–ª—ã
    time.sleep(1)  # –ß—Ç–æ–±—ã –≤—Ä–µ–º—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ç–ª–∏—á–∞–ª–æ—Å—å
    
    # –ò–∑–º–µ–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª
    with open(test_source / "readme.txt", 'a', encoding='utf-8') as f:
        f.write("\n–î–æ–±–∞–≤–ª–µ–Ω–∞ –Ω–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª
    with open(test_source / "new_file.txt", 'w', encoding='utf-8') as f:
        f.write("–≠—Ç–æ –Ω–æ–≤—ã–π —Ñ–∞–π–ª, —Å–æ–∑–¥–∞–Ω–Ω—ã–π –ø–æ—Å–ª–µ –ø–æ–ª–Ω–æ–π —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏")
    
    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª
    (test_source / "images" / "placeholder.txt").unlink()
    
    print("–§–∞–π–ª—ã –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    print("\n5. –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏:")
    
    incremental_backup_name = backup_system.create_incremental_backup(test_source)
    
    print("\n6. –°–ø–∏—Å–æ–∫ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π:")
    
    backup_system.list_backups()
    
    print("\n7. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏:")
    
    backup_system.verify_backup(full_backup_name)
    if incremental_backup_name:
        backup_system.verify_backup(incremental_backup_name)
    
    print("\n8. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏:")
    
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–ª–Ω—É—é –∫–æ–ø–∏—é
    restore_dir_full = "restored_full"
    backup_system.restore_backup(full_backup_name, restore_dir_full)
    
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—É—é –∫–æ–ø–∏—é
    if incremental_backup_name:
        restore_dir_incremental = "restored_incremental"
        backup_system.restore_backup(incremental_backup_name, restore_dir_incremental)
    
    print("\n9. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã:")
    
    backup_dir_size = sum(f.stat().st_size for f in Path("test_backups").rglob("*") if f.is_file())
    
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è:")
    print(f"  –í—Å–µ–≥–æ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π: {len(backup_system.metadata['backups'])}")
    print(f"  –†–∞–∑–º–µ—Ä –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π: {backup_dir_size:,} –±–∞–π—Ç")
    print(f"  –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ª–Ω–∞—è –∫–æ–ø–∏—è: {backup_system.metadata['last_full_backup']}")
    
    print("\n10. –û—á–∏—Å—Ç–∫–∞:")
    
    # –£–¥–∞–ª—è–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ —Ñ–∞–π–ª—ã
    cleanup_dirs = [test_source, "test_backups", restore_dir_full]
    if incremental_backup_name:
        cleanup_dirs.append("restored_incremental")
    
    for directory in cleanup_dirs:
        if Path(directory).exists():
            shutil.rmtree(directory)
            print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {directory}")
    
    print("‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –≤—Å–µ—Ö —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π
    """
    exercises = [
        ("–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥-—Ñ–∞–π–ª–æ–≤", exercise_01_log_analyzer),
        ("–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä –¥–∞–Ω–Ω—ã—Ö", exercise_02_data_converter),
        ("–°–∏—Å—Ç–µ–º–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è", exercise_03_backup_system),
    ]
    
    print("üìÅ –£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è: –†–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–∞–º–∏ –≤ Python")
    print("=" * 70)
    print("–≠—Ç–∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –ø–æ–º–æ–≥—É—Ç –æ—Å–≤–æ–∏—Ç—å:")
    print("- –û–±—Ä–∞–±–æ—Ç–∫—É –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤")
    print("- –ê–Ω–∞–ª–∏–∑ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö")
    print("- –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é –º–µ–∂–¥—É —Ñ–æ—Ä–º–∞—Ç–∞–º–∏")
    print("- –°–∏—Å—Ç–µ–º—ã —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è")
    print("- –†–∞–±–æ—Ç—É —Å –∞—Ä—Ö–∏–≤–∞–º–∏ –∏ —Å–∂–∞—Ç–∏–µ–º")
    print("- –ü—Ä–æ–≤–µ—Ä–∫—É —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 70)
    
    for i, (name, func) in enumerate(exercises, 1):
        print(f"\n{i}. {name}")
        print("-" * (len(name) + 3))
        try:
            func()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è: {e}")
            import traceback
            traceback.print_exc()
        
        if i < len(exercises):
            input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è...")


if __name__ == "__main__":
    main() 