# –¢–µ–æ—Ä–∏—è: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ Python

## üéØ –¶–µ–ª—å —Ä–∞–∑–¥–µ–ª–∞

–≠—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∞—Å–ø–µ–∫—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ Python: –æ—Ç –±–∞–∑–æ–≤–æ–π —Ä–∞–±–æ—Ç—ã —Å Pandas –¥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π.

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [Pandas - –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö](#pandas---–∞–Ω–∞–ª–∏–∑-–¥–∞–Ω–Ω—ã—Ö)
2. [NumPy - —á–∏—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è](#numpy---—á–∏—Å–ª–µ–Ω–Ω—ã–µ-–≤—ã—á–∏—Å–ª–µ–Ω–∏—è)
3. [–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö](#–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è-–¥–∞–Ω–Ω—ã—Ö)
4. [–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑](#—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π-–∞–Ω–∞–ª–∏–∑)
5. [–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å Scikit-learn](#–º–∞—à–∏–Ω–Ω–æ–µ-–æ–±—É—á–µ–Ω–∏–µ-—Å-scikit-learn)
6. [Deep Learning –æ—Å–Ω–æ–≤—ã](#deep-learning-–æ—Å–Ω–æ–≤—ã)
7. [MLOps –∏ –ø—Ä–æ–¥–∞–∫—à–Ω](#mlops-–∏-–ø—Ä–æ–¥–∞–∫—à–Ω)

---

## üêº Pandas - –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö

Pandas - –æ—Å–Ω–æ–≤–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ Python.

### –û—Å–Ω–æ–≤—ã —Ä–∞–±–æ—Ç—ã —Å DataFrame

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Union, Tuple
import warnings
warnings.filterwarnings('ignore')

class DataAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏"""
    
    def __init__(self, data: Union[str, pd.DataFrame, Dict] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            data: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É, DataFrame –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏
        """
        self.df = None
        self.metadata = {}
        
        if data is not None:
            self.load_data(data)
    
    def load_data(self, data: Union[str, pd.DataFrame, Dict]):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        if isinstance(data, str):
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞
            if data.endswith('.csv'):
                self.df = pd.read_csv(data)
            elif data.endswith('.xlsx') or data.endswith('.xls'):
                self.df = pd.read_excel(data)
            elif data.endswith('.json'):
                self.df = pd.read_json(data)
            elif data.endswith('.parquet'):
                self.df = pd.read_parquet(data)
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {data}")
                
        elif isinstance(data, pd.DataFrame):
            self.df = data.copy()
            
        elif isinstance(data, dict):
            self.df = pd.DataFrame(data)
            
        else:
            raise TypeError("–î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—É—Ç–µ–º –∫ —Ñ–∞–π–ª—É, DataFrame –∏–ª–∏ —Å–ª–æ–≤–∞—Ä–µ–º")
        
        self._update_metadata()
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {self.df.shape[0]} —Å—Ç—Ä–æ–∫, {self.df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
    
    def _update_metadata(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        if self.df is not None:
            self.metadata = {
                'shape': self.df.shape,
                'columns': list(self.df.columns),
                'dtypes': dict(self.df.dtypes),
                'memory_usage': self.df.memory_usage(deep=True).sum(),
                'missing_values': dict(self.df.isnull().sum()),
                'created_at': datetime.now()
            }
    
    def basic_info(self) -> Dict[str, Any]:
        """–ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        if self.df is None:
            return {}
        
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        categorical_cols = self.df.select_dtypes(include=['object']).columns
        datetime_cols = self.df.select_dtypes(include=['datetime64']).columns
        
        return {
            'basic_stats': {
                'rows': len(self.df),
                'columns': len(self.df.columns),
                'memory_usage_mb': self.df.memory_usage(deep=True).sum() / (1024 * 1024),
                'missing_values_total': self.df.isnull().sum().sum(),
                'missing_percentage': (self.df.isnull().sum().sum() / (len(self.df) * len(self.df.columns))) * 100
            },
            'column_types': {
                'numeric': len(numeric_cols),
                'categorical': len(categorical_cols),
                'datetime': len(datetime_cols)
            },
            'data_quality': {
                'duplicated_rows': self.df.duplicated().sum(),
                'unique_rows': len(self.df.drop_duplicates()),
                'columns_with_nulls': (self.df.isnull().sum() > 0).sum()
            }
        }
    
    def describe_data(self, include_all: bool = True) -> pd.DataFrame:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
        if self.df is None:
            return pd.DataFrame()
        
        if include_all:
            return self.df.describe(include='all')
        else:
            return self.df.describe()
    
    def missing_values_analysis(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        if self.df is None:
            return {}
        
        missing = self.df.isnull().sum()
        missing_percent = (missing / len(self.df)) * 100
        
        missing_df = pd.DataFrame({
            'column': missing.index,
            'missing_count': missing.values,
            'missing_percentage': missing_percent.values
        }).sort_values('missing_percentage', ascending=False)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        patterns = {}
        for col in self.df.columns:
            if self.df[col].isnull().sum() > 0:
                pattern = self.df[col].isnull().value_counts()
                patterns[col] = {
                    'has_missing': pattern.get(True, 0),
                    'no_missing': pattern.get(False, 0)
                }
        
        return {
            'summary': missing_df,
            'patterns': patterns,
            'recommendations': self._get_missing_value_recommendations(missing_df)
        }
    
    def _get_missing_value_recommendations(self, missing_df: pd.DataFrame) -> List[str]:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        recommendations = []
        
        for _, row in missing_df.iterrows():
            col, missing_count, missing_pct = row['column'], row['missing_count'], row['missing_percentage']
            
            if missing_pct == 0:
                continue
            elif missing_pct < 5:
                recommendations.append(f"{col}: –ú–∞–ª–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ ({missing_pct:.1f}%), –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ –∑–∞–ø–æ–ª–Ω–∏—Ç—å")
            elif missing_pct < 30:
                recommendations.append(f"{col}: –£–º–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–æ–ø—É—Å–∫–∏ ({missing_pct:.1f}%), —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏–º–ø—É—Ç–∞—Ü–∏—è")
            elif missing_pct < 70:
                recommendations.append(f"{col}: –ú–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ ({missing_pct:.1f}%), —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞")
            else:
                recommendations.append(f"{col}: –ö—Ä–∏—Ç–∏—á–Ω–æ –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ ({missing_pct:.1f}%), —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–¥–∞–ª–∏—Ç—å")
        
        return recommendations
    
    def outliers_analysis(self, method: str = 'iqr') -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤"""
        if self.df is None:
            return {}
        
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        outliers_info = {}
        
        for col in numeric_cols:
            if method == 'iqr':
                Q1 = self.df[col].quantile(0.25)
                Q3 = self.df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)]
                
            elif method == 'zscore':
                z_scores = np.abs((self.df[col] - self.df[col].mean()) / self.df[col].std())
                outliers = self.df[z_scores > 3]
            
            outliers_info[col] = {
                'count': len(outliers),
                'percentage': (len(outliers) / len(self.df)) * 100,
                'values': outliers[col].tolist() if len(outliers) < 20 else outliers[col].head(20).tolist()
            }
        
        return outliers_info
    
    def correlation_analysis(self, method: str = 'pearson') -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π"""
        if self.df is None:
            return {}
        
        numeric_df = self.df.select_dtypes(include=[np.number])
        
        if len(numeric_df.columns) < 2:
            return {'error': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏'}
        
        correlation_matrix = numeric_df.corr(method=method)
        
        # –ù–∞–π–¥–µ–º —Å–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        strong_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:  # –°–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
                    strong_correlations.append({
                        'feature1': correlation_matrix.columns[i],
                        'feature2': correlation_matrix.columns[j],
                        'correlation': corr_value
                    })
        
        return {
            'matrix': correlation_matrix,
            'strong_correlations': strong_correlations,
            'summary': f"–ù–∞–π–¥–µ–Ω–æ {len(strong_correlations)} —Å–∏–ª—å–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π (|r| > 0.7)"
        }
    
    def categorical_analysis(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        if self.df is None:
            return {}
        
        categorical_cols = self.df.select_dtypes(include=['object']).columns
        analysis = {}
        
        for col in categorical_cols:
            value_counts = self.df[col].value_counts()
            
            analysis[col] = {
                'unique_values': self.df[col].nunique(),
                'most_frequent': value_counts.index[0] if len(value_counts) > 0 else None,
                'most_frequent_count': value_counts.iloc[0] if len(value_counts) > 0 else 0,
                'distribution': value_counts.head(10).to_dict(),  # –¢–æ–ø-10
                'cardinality': self._assess_cardinality(self.df[col].nunique(), len(self.df))
            }
        
        return analysis
    
    def _assess_cardinality(self, unique_count: int, total_count: int) -> str:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
        ratio = unique_count / total_count
        
        if ratio < 0.05:
            return "–ù–∏–∑–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å"
        elif ratio < 0.5:
            return "–°—Ä–µ–¥–Ω—è—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å"
        else:
            return "–í—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å"
    
    def time_series_analysis(self, date_column: str, value_column: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
        if self.df is None or date_column not in self.df.columns or value_column not in self.df.columns:
            return {}
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ datetime –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if not pd.api.types.is_datetime64_any_dtype(self.df[date_column]):
            self.df[date_column] = pd.to_datetime(self.df[date_column])
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
        ts_df = self.df.sort_values(date_column)
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        date_range = ts_df[date_column].max() - ts_df[date_column].min()
        frequency = self._detect_frequency(ts_df[date_column])
        
        # –¢—Ä–µ–Ω–¥ –∞–Ω–∞–ª–∏–∑ (–ø—Ä–æ—Å—Ç–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è)
        from scipy import stats
        x = np.arange(len(ts_df))
        y = ts_df[value_column].values
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        
        # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
        seasonality = self._detect_seasonality(ts_df, date_column, value_column)
        
        return {
            'basic_stats': {
                'start_date': ts_df[date_column].min(),
                'end_date': ts_df[date_column].max(),
                'date_range': str(date_range),
                'data_points': len(ts_df),
                'frequency': frequency
            },
            'trend': {
                'slope': slope,
                'r_squared': r_value**2,
                'p_value': p_value,
                'trend_direction': '–≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–∏–π' if slope > 0 else '—É–±—ã–≤–∞—é—â–∏–π' if slope < 0 else '—Å—Ç–∞–±–∏–ª—å–Ω—ã–π'
            },
            'seasonality': seasonality,
            'missing_dates': self._find_missing_dates(ts_df, date_column, frequency)
        }
    
    def _detect_frequency(self, date_series: pd.Series) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞"""
        if len(date_series) < 2:
            return "unknown"
        
        diffs = date_series.diff().dropna()
        mode_diff = diffs.mode().iloc[0] if len(diffs.mode()) > 0 else diffs.median()
        
        if mode_diff <= timedelta(days=1):
            return "daily"
        elif mode_diff <= timedelta(days=7):
            return "weekly"
        elif mode_diff <= timedelta(days=31):
            return "monthly"
        else:
            return "irregular"
    
    def _detect_seasonality(self, df: pd.DataFrame, date_col: str, value_col: str) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏"""
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        df_temp = df.copy()
        df_temp['month'] = df_temp[date_col].dt.month
        df_temp['day_of_week'] = df_temp[date_col].dt.dayofweek
        df_temp['quarter'] = df_temp[date_col].dt.quarter
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞—Ü–∏—é –ø–æ –º–µ—Å—è—Ü–∞–º
        monthly_stats = df_temp.groupby('month')[value_col].agg(['mean', 'std']).reset_index()
        monthly_cv = (monthly_stats['std'] / monthly_stats['mean']).mean()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞—Ü–∏—é –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
        weekly_stats = df_temp.groupby('day_of_week')[value_col].agg(['mean', 'std']).reset_index()
        weekly_cv = (weekly_stats['std'] / weekly_stats['mean']).mean()
        
        return {
            'monthly_seasonality': monthly_cv > 0.1,
            'weekly_seasonality': weekly_cv > 0.1,
            'monthly_variation': monthly_cv,
            'weekly_variation': weekly_cv
        }
    
    def _find_missing_dates(self, df: pd.DataFrame, date_col: str, frequency: str) -> List[str]:
        """–ü–æ–∏—Å–∫ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞—Ç"""
        if frequency == "daily":
            full_range = pd.date_range(start=df[date_col].min(), end=df[date_col].max(), freq='D')
        elif frequency == "weekly":
            full_range = pd.date_range(start=df[date_col].min(), end=df[date_col].max(), freq='W')
        elif frequency == "monthly":
            full_range = pd.date_range(start=df[date_col].min(), end=df[date_col].max(), freq='MS')
        else:
            return []
        
        existing_dates = set(df[date_col].dt.date)
        full_dates = set(full_range.date)
        missing_dates = full_dates - existing_dates
        
        return [str(date) for date in sorted(missing_dates)]

# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
class AdvancedDataProcessor:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()
        self.transformations_log = []
    
    def handle_missing_values(self, strategy: Dict[str, str]) -> pd.DataFrame:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        
        Args:
            strategy: –°–ª–æ–≤–∞—Ä—å {column: method} –≥–¥–µ method –º–æ–∂–µ—Ç –±—ã—Ç—å:
                     'drop', 'mean', 'median', 'mode', 'forward_fill', 'backward_fill', 'constant'
        """
        df_processed = self.df.copy()
        
        for column, method in strategy.items():
            if column not in df_processed.columns:
                continue
                
            if method == 'drop':
                df_processed = df_processed.dropna(subset=[column])
                
            elif method == 'mean' and pd.api.types.is_numeric_dtype(df_processed[column]):
                df_processed[column] = df_processed[column].fillna(df_processed[column].mean())
                
            elif method == 'median' and pd.api.types.is_numeric_dtype(df_processed[column]):
                df_processed[column] = df_processed[column].fillna(df_processed[column].median())
                
            elif method == 'mode':
                mode_value = df_processed[column].mode().iloc[0] if len(df_processed[column].mode()) > 0 else 0
                df_processed[column] = df_processed[column].fillna(mode_value)
                
            elif method == 'forward_fill':
                df_processed[column] = df_processed[column].fillna(method='ffill')
                
            elif method == 'backward_fill':
                df_processed[column] = df_processed[column].fillna(method='bfill')
                
            elif method.startswith('constant_'):
                constant_value = method.split('_', 1)[1]
                df_processed[column] = df_processed[column].fillna(constant_value)
            
            self.transformations_log.append(f"Missing values in {column} handled with {method}")
        
        return df_processed
    
    def handle_outliers(self, columns: List[str], method: str = 'iqr', action: str = 'remove') -> pd.DataFrame:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
        
        Args:
            columns: –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            method: 'iqr' –∏–ª–∏ 'zscore'
            action: 'remove', 'cap', 'transform'
        """
        df_processed = self.df.copy()
        
        for column in columns:
            if column not in df_processed.columns or not pd.api.types.is_numeric_dtype(df_processed[column]):
                continue
            
            if method == 'iqr':
                Q1 = df_processed[column].quantile(0.25)
                Q3 = df_processed[column].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                if action == 'remove':
                    df_processed = df_processed[
                        (df_processed[column] >= lower_bound) & 
                        (df_processed[column] <= upper_bound)
                    ]
                elif action == 'cap':
                    df_processed[column] = np.clip(df_processed[column], lower_bound, upper_bound)
                elif action == 'transform':
                    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    if (df_processed[column] > 0).all():
                        df_processed[column] = np.log1p(df_processed[column])
            
            elif method == 'zscore':
                z_scores = np.abs((df_processed[column] - df_processed[column].mean()) / df_processed[column].std())
                
                if action == 'remove':
                    df_processed = df_processed[z_scores <= 3]
                elif action == 'cap':
                    mean_val = df_processed[column].mean()
                    std_val = df_processed[column].std()
                    lower_bound = mean_val - 3 * std_val
                    upper_bound = mean_val + 3 * std_val
                    df_processed[column] = np.clip(df_processed[column], lower_bound, upper_bound)
            
            self.transformations_log.append(f"Outliers in {column} handled with {method}-{action}")
        
        return df_processed
    
    def encode_categorical(self, columns: List[str], method: str = 'onehot') -> pd.DataFrame:
        """
        –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        
        Args:
            columns: –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            method: 'onehot', 'label', 'target'
        """
        df_processed = self.df.copy()
        
        for column in columns:
            if column not in df_processed.columns:
                continue
            
            if method == 'onehot':
                # One-hot encoding
                dummies = pd.get_dummies(df_processed[column], prefix=column)
                df_processed = pd.concat([df_processed.drop(column, axis=1), dummies], axis=1)
                
            elif method == 'label':
                # Label encoding
                from sklearn.preprocessing import LabelEncoder
                le = LabelEncoder()
                df_processed[column] = le.fit_transform(df_processed[column].astype(str))
                
            self.transformations_log.append(f"Categorical encoding for {column} with {method}")
        
        return df_processed
    
    def scale_features(self, columns: List[str], method: str = 'standard') -> pd.DataFrame:
        """
        –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            columns: –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
            method: 'standard', 'minmax', 'robust'
        """
        df_processed = self.df.copy()
        
        for column in columns:
            if column not in df_processed.columns or not pd.api.types.is_numeric_dtype(df_processed[column]):
                continue
            
            if method == 'standard':
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è (z-score)
                df_processed[column] = (df_processed[column] - df_processed[column].mean()) / df_processed[column].std()
                
            elif method == 'minmax':
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è min-max
                min_val = df_processed[column].min()
                max_val = df_processed[column].max()
                df_processed[column] = (df_processed[column] - min_val) / (max_val - min_val)
                
            elif method == 'robust':
                # –†–æ–±–∞—Å—Ç–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                median = df_processed[column].median()
                q75 = df_processed[column].quantile(0.75)
                q25 = df_processed[column].quantile(0.25)
                iqr = q75 - q25
                df_processed[column] = (df_processed[column] - median) / iqr
            
            self.transformations_log.append(f"Feature scaling for {column} with {method}")
        
        return df_processed
    
    def create_features(self, feature_config: Dict[str, Any]) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            feature_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        df_processed = self.df.copy()
        
        for feature_name, config in feature_config.items():
            feature_type = config.get('type')
            
            if feature_type == 'polynomial':
                # –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                column = config['column']
                degree = config.get('degree', 2)
                if column in df_processed.columns:
                    df_processed[feature_name] = df_processed[column] ** degree
            
            elif feature_type == 'interaction':
                # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                columns = config['columns']
                if all(col in df_processed.columns for col in columns):
                    df_processed[feature_name] = df_processed[columns].prod(axis=1)
            
            elif feature_type == 'binning':
                # –ë–∏–Ω–Ω–∏–Ω–≥
                column = config['column']
                bins = config.get('bins', 5)
                if column in df_processed.columns:
                    df_processed[feature_name] = pd.cut(df_processed[column], bins=bins, labels=False)
            
            elif feature_type == 'datetime':
                # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                column = config['column']
                if column in df_processed.columns:
                    df_processed[column] = pd.to_datetime(df_processed[column])
                    
                    if 'year' in config.get('extract', []):
                        df_processed[f"{feature_name}_year"] = df_processed[column].dt.year
                    if 'month' in config.get('extract', []):
                        df_processed[f"{feature_name}_month"] = df_processed[column].dt.month
                    if 'day' in config.get('extract', []):
                        df_processed[f"{feature_name}_day"] = df_processed[column].dt.day
                    if 'dayofweek' in config.get('extract', []):
                        df_processed[f"{feature_name}_dayofweek"] = df_processed[column].dt.dayofweek
            
            self.transformations_log.append(f"Feature {feature_name} created with type {feature_type}")
        
        return df_processed
    
    def get_transformation_summary(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π"""
        return self.transformations_log.copy()

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
class DataBaseConnector:
    """–ö–æ–Ω–Ω–µ–∫—Ç–æ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, connection_string: str):
        """
        Args:
            connection_string: –°—Ç—Ä–æ–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î
        """
        self.connection_string = connection_string
        self.engine = None
        self._connect()
    
    def _connect(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
        try:
            from sqlalchemy import create_engine
            self.engine = create_engine(self.connection_string)
            print("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
    
    def read_table(self, table_name: str, limit: Optional[int] = None) -> pd.DataFrame:
        """–ß—Ç–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        query = f"SELECT * FROM {table_name}"
        if limit:
            query += f" LIMIT {limit}"
        
        try:
            return pd.read_sql(query, self.engine)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã {table_name}: {e}")
            return pd.DataFrame()
    
    def execute_query(self, query: str) -> pd.DataFrame:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ SQL –∑–∞–ø—Ä–æ—Å–∞"""
        try:
            return pd.read_sql(query, self.engine)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return pd.DataFrame()
    
    def write_table(self, df: pd.DataFrame, table_name: str, 
                   if_exists: str = 'replace', index: bool = False) -> bool:
        """–ó–∞–ø–∏—Å—å DataFrame –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            df.to_sql(table_name, self.engine, if_exists=if_exists, index=index)
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}: {e}")
            return False
    
    def get_table_info(self, table_name: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–∞–±–ª–∏—Ü–µ"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ö–µ–º—É —Ç–∞–±–ª–∏—Ü—ã
            columns_query = f"""
            SELECT column_name, data_type, is_nullable
            FROM information_schema.columns 
            WHERE table_name = '{table_name}'
            """
            columns_info = pd.read_sql(columns_query, self.engine)
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
            count_query = f"SELECT COUNT(*) as row_count FROM {table_name}"
            row_count = pd.read_sql(count_query, self.engine)['row_count'].iloc[0]
            
            return {
                'columns': columns_info.to_dict('records'),
                'row_count': row_count,
                'table_name': table_name
            }
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–∞–±–ª–∏—Ü–µ: {e}")
            return {}

# –ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
class DataAnalysisPipeline:
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, data_source: Union[str, pd.DataFrame]):
        self.analyzer = DataAnalyzer(data_source)
        self.processor = AdvancedDataProcessor(self.analyzer.df)
        self.results = {}
    
    def run_full_analysis(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("üîç –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        self.results['basic_info'] = self.analyzer.basic_info()
        print("‚úÖ –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ–±—Ä–∞–Ω–∞")
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        self.results['missing_values'] = self.analyzer.missing_values_analysis()
        print("‚úÖ –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω")
        
        # –ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤
        self.results['outliers'] = self.analyzer.outliers_analysis()
        print("‚úÖ –ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω")
        
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        self.results['correlations'] = self.analyzer.correlation_analysis()
        print("‚úÖ –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        self.results['categorical'] = self.analyzer.categorical_analysis()
        print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω")
        
        print("üéâ –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω!")
        return self.results
    
    def generate_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        if not self.results:
            return "–ê–Ω–∞–ª–∏–∑ –Ω–µ –±—ã–ª –≤—ã–ø–æ–ª–Ω–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ run_full_analysis() —Å–Ω–∞—á–∞–ª–∞."
        
        report = []
        report.append("üìä –û–¢–ß–ï–¢ –ü–û –ê–ù–ê–õ–ò–ó–£ –î–ê–ù–ù–´–•")
        report.append("=" * 50)
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        basic = self.results.get('basic_info', {})
        if basic:
            stats = basic.get('basic_stats', {})
            report.append(f"\nüìã –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
            report.append(f"  ‚Ä¢ –°—Ç—Ä–æ–∫–∏: {stats.get('rows', 'N/A'):,}")
            report.append(f"  ‚Ä¢ –°—Ç–æ–ª–±—Ü—ã: {stats.get('columns', 'N/A')}")
            report.append(f"  ‚Ä¢ –†–∞–∑–º–µ—Ä –≤ –ø–∞–º—è—Ç–∏: {stats.get('memory_usage_mb', 0):.1f} MB")
            report.append(f"  ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {stats.get('missing_values_total', 0):,}")
            report.append(f"  ‚Ä¢ –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏: {basic.get('data_quality', {}).get('duplicated_rows', 0):,}")
        
        # –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        missing = self.results.get('missing_values', {})
        if missing and 'recommendations' in missing:
            report.append(f"\nüîç –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º:")
            for rec in missing['recommendations'][:5]:  # –¢–æ–ø-5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
                report.append(f"  ‚Ä¢ {rec}")
        
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        corr = self.results.get('correlations', {})
        if corr and 'strong_correlations' in corr:
            strong_corr = corr['strong_correlations']
            if strong_corr:
                report.append(f"\nüîó –°–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (|r| > 0.7):")
                for item in strong_corr[:5]:  # –¢–æ–ø-5 –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
                    report.append(f"  ‚Ä¢ {item['feature1']} ‚Üî {item['feature2']}: {item['correlation']:.3f}")
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        cat = self.results.get('categorical', {})
        if cat:
            report.append(f"\nüìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:")
            for col, info in list(cat.items())[:5]:  # –¢–æ–ø-5 —Å—Ç–æ–ª–±—Ü–æ–≤
                report.append(f"  ‚Ä¢ {col}: {info['unique_values']} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ({info['cardinality']})")
        
        return "\n".join(report)

# –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def create_sample_dataset():
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    np.random.seed(42)
    n_samples = 1000
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
    data = {
        'customer_id': range(1, n_samples + 1),
        'age': np.random.normal(35, 12, n_samples).astype(int),
        'income': np.random.exponential(50000, n_samples),
        'spending_score': np.random.beta(2, 5, n_samples) * 100,
        'city': np.random.choice(['–ú–æ—Å–∫–≤–∞', '–°–ü–±', '–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥', '–ö–∞–∑–∞–Ω—å', '–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫'], n_samples),
        'gender': np.random.choice(['–ú', '–ñ'], n_samples),
        'purchase_date': pd.date_range('2023-01-01', periods=n_samples, freq='H'),
        'category': np.random.choice(['–≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞', '–û–¥–µ–∂–¥–∞', '–ï–¥–∞', '–ö–Ω–∏–≥–∏', '–°–ø–æ—Ä—Ç'], n_samples)
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    df = pd.DataFrame(data)
    df.loc[np.random.choice(df.index, 50), 'income'] = np.nan
    df.loc[np.random.choice(df.index, 30), 'city'] = np.nan
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—ã–±—Ä–æ—Å–æ–≤
    df.loc[np.random.choice(df.index, 10), 'income'] = df['income'].max() * 3
    
    return df

def demonstrate_analysis_pipeline():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞"""
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    df = create_sample_dataset()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
    pipeline = DataAnalysisPipeline(df)
    results = pipeline.run_full_analysis()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report = pipeline.generate_report()
    print("\n" + report)
    
    return pipeline, results
```

---

## üî¢ NumPy - —á–∏—Å–ª–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è

NumPy - –æ—Å–Ω–æ–≤–∞ –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –º–∞—Å—Å–∏–≤–æ–≤ –≤ Python.

### –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –º–∞—Å—Å–∏–≤–∞–º–∏

```python
import numpy as np
from typing import Tuple, List, Union, Optional, Callable
import time
from scipy import linalg, sparse
from numba import jit, vectorize
import matplotlib.pyplot as plt

class AdvancedNumPyOperations:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å NumPy"""
    
    @staticmethod
    def create_structured_arrays():
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–∞—Å—Å–∏–≤–æ–≤"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
        dtype = np.dtype([
            ('name', 'U20'),
            ('age', 'i4'),
            ('height', 'f4'),
            ('weight', 'f4'),
            ('salary', 'f8')
        ])
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—Å–∏–≤
        people = np.array([
            ('Alice', 25, 165.5, 55.2, 50000.0),
            ('Bob', 30, 180.0, 75.8, 60000.0),
            ('Charlie', 35, 175.2, 70.1, 75000.0)
        ], dtype=dtype)
        
        return people
    
    @staticmethod
    def memory_optimization_example():
        """–ü—Ä–∏–º–µ—Ä –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏"""
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        sizes = [1000000]
        
        results = {}
        for size in sizes:
            # float64 (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
            arr_64 = np.random.random(size).astype(np.float64)
            memory_64 = arr_64.nbytes / (1024 * 1024)  # –ú–ë
            
            # float32
            arr_32 = arr_64.astype(np.float32)
            memory_32 = arr_32.nbytes / (1024 * 1024)
            
            # int16 –¥–ª—è —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ -32768..32767
            if np.all(arr_64.astype(int) == arr_64) and np.all(arr_64 >= -32768) and np.all(arr_64 <= 32767):
                arr_16 = arr_64.astype(np.int16)
                memory_16 = arr_16.nbytes / (1024 * 1024)
            else:
                memory_16 = None
            
            results[size] = {
                'float64': memory_64,
                'float32': memory_32,
                'int16': memory_16,
                'savings_32': (memory_64 - memory_32) / memory_64 * 100
            }
        
        return results
    
    @staticmethod
    def vectorization_examples():
        """–ü—Ä–∏–º–µ—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"""
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        size = 1000000
        x = np.random.random(size)
        y = np.random.random(size)
        
        # –ü—Ä–∏–º–µ—Ä 1: –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Å–ø–æ—Å–æ–± (—Ü–∏–∫–ª—ã Python)
        def slow_calculation(x, y):
            result = np.zeros_like(x)
            for i in range(len(x)):
                result[i] = np.sqrt(x[i]**2 + y[i]**2) * np.sin(x[i]) + np.cos(y[i])
            return result
        
        # –ü—Ä–∏–º–µ—Ä 2: –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–ø–æ—Å–æ–±
        def fast_calculation(x, y):
            return np.sqrt(x**2 + y**2) * np.sin(x) + np.cos(y)
        
        # –ü—Ä–∏–º–µ—Ä 3: Ufunc —Å –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–º
        @vectorize(['float64(float64, float64)'], target='cpu')
        def numba_calculation(x, y):
            return np.sqrt(x**2 + y**2) * np.sin(x) + np.cos(y)
        
        # –ó–∞–º–µ—Ä—ã –≤—Ä–µ–º–µ–Ω–∏
        times = {}
        
        # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Å–ø–æ—Å–æ–±
        start = time.time()
        result_fast = fast_calculation(x, y)
        times['vectorized'] = time.time() - start
        
        # Numba ufunc
        start = time.time()
        result_numba = numba_calculation(x, y)
        times['numba'] = time.time() - start
        
        # –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Å–ø–æ—Å–æ–± (—Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–±–æ–ª—å—à–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞)
        if size <= 10000:
            start = time.time()
            result_slow = slow_calculation(x[:10000], y[:10000])
            times['loops'] = time.time() - start
        
        return {
            'times': times,
            'speedup': times.get('loops', times['vectorized']) / times['vectorized'] if 'loops' in times else times['numba'] / times['vectorized']
        }
    
    @staticmethod
    def advanced_indexing_examples():
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è"""
        # –°–æ–∑–¥–∞–µ–º 3D –º–∞—Å—Å–∏–≤
        arr_3d = np.random.randint(0, 100, (5, 6, 7))
        
        examples = {}
        
        # Fancy indexing
        examples['fancy_indexing'] = {
            'description': '–í—ã–±–æ—Ä –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º',
            'code': 'arr[[0, 2, 4], [1, 3, 5]]',
            'result': arr_3d[[0, 2, 4], [1, 3, 5]].shape
        }
        
        # Boolean indexing
        mask = arr_3d > 50
        examples['boolean_indexing'] = {
            'description': '–í—ã–±–æ—Ä —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ —É—Å–ª–æ–≤–∏—é',
            'code': 'arr[arr > 50]',
            'count': np.sum(mask)
        }
        
        # Mesh grid indexing
        x_idx, y_idx = np.meshgrid([0, 2, 4], [1, 3, 5])
        examples['meshgrid_indexing'] = {
            'description': '–í—ã–±–æ—Ä –±–ª–æ–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é meshgrid',
            'shape': arr_3d[x_idx, y_idx].shape
        }
        
        # Advanced slicing
        examples['advanced_slicing'] = {
            'description': '–°–ª–æ–∂–Ω—ã–µ —Å—Ä–µ–∑—ã',
            'every_other': arr_3d[::2, ::2, ::2].shape,
            'reverse': arr_3d[::-1, ::-1, ::-1].shape
        }
        
        return examples
    
    @staticmethod
    def broadcasting_examples():
        """–ü—Ä–∏–º–µ—Ä—ã broadcasting"""
        examples = {}
        
        # –ë–∞–∑–æ–≤—ã–π broadcasting
        a = np.array([[1, 2, 3],
                     [4, 5, 6]])  # (2, 3)
        b = np.array([10, 20, 30])  # (3,)
        
        examples['basic'] = {
            'a_shape': a.shape,
            'b_shape': b.shape,
            'result_shape': (a + b).shape,
            'result': a + b
        }
        
        # –°–ª–æ–∂–Ω—ã–π broadcasting
        c = np.random.random((5, 1, 3))  # (5, 1, 3)
        d = np.random.random((1, 4, 1))  # (1, 4, 1)
        
        examples['complex'] = {
            'c_shape': c.shape,
            'd_shape': d.shape,
            'result_shape': (c + d).shape
        }
        
        # –û—à–∏–±–∫–∏ broadcasting
        try:
            e = np.random.random((3, 2))
            f = np.random.random((4, 2))
            result = e + f  # –î–æ–ª–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å –æ—à–∏–±–∫—É
        except ValueError as error:
            examples['error'] = {
                'error_message': str(error),
                'incompatible_shapes': (e.shape, f.shape)
            }
        
        return examples

class LinearAlgebraOperations:
    """–û–ø–µ—Ä–∞—Ü–∏–∏ –ª–∏–Ω–µ–π–Ω–æ–π –∞–ª–≥–µ–±—Ä—ã"""
    
    @staticmethod
    def matrix_operations_comparison():
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–ø–æ—Å–æ–±–æ–≤ –º–∞—Ç—Ä–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"""
        size = 1000
        A = np.random.random((size, size))
        B = np.random.random((size, size))
        
        operations = {}
        
        # –£–º–Ω–æ–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü
        start = time.time()
        C1 = np.dot(A, B)
        operations['np.dot'] = time.time() - start
        
        start = time.time()
        C2 = A @ B
        operations['@ operator'] = time.time() - start
        
        start = time.time()
        C3 = np.matmul(A, B)
        operations['np.matmul'] = time.time() - start
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ
        operations['results_equal'] = np.allclose(C1, C2) and np.allclose(C2, C3)
        
        return operations
    
    @staticmethod
    def solve_linear_systems():
        """–†–µ—à–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º –ª–∏–Ω–µ–π–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π"""
        # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É Ax = b
        n = 100
        A = np.random.random((n, n))
        true_x = np.random.random(n)
        b = A @ true_x
        
        solutions = {}
        
        # –ú–µ—Ç–æ–¥ 1: np.linalg.solve
        start = time.time()
        x1 = np.linalg.solve(A, b)
        solutions['linalg.solve'] = {
            'time': time.time() - start,
            'error': np.linalg.norm(x1 - true_x)
        }
        
        # –ú–µ—Ç–æ–¥ 2: scipy.linalg.solve (—á–∞—Å—Ç–æ –±—ã—Å—Ç—Ä–µ–µ)
        start = time.time()
        x2 = linalg.solve(A, b)
        solutions['scipy.solve'] = {
            'time': time.time() - start,
            'error': np.linalg.norm(x2 - true_x)
        }
        
        # –ú–µ—Ç–æ–¥ 3: –ø—Å–µ–≤–¥–æ–æ–±—Ä–∞—Ç–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (–¥–ª—è –ø–ª–æ—Ö–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü)
        start = time.time()
        x3 = np.linalg.pinv(A) @ b
        solutions['pinv'] = {
            'time': time.time() - start,
            'error': np.linalg.norm(x3 - true_x)
        }
        
        return solutions
    
    @staticmethod
    def eigenvalue_analysis():
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –≤–µ–∫—Ç–æ—Ä–æ–≤"""
        # –°–æ–∑–¥–∞–µ–º —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
        n = 100
        A = np.random.random((n, n))
        A = (A + A.T) / 2  # –î–µ–ª–∞–µ–º —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–π
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –≤–µ–∫—Ç–æ—Ä—ã
        eigenvalues, eigenvectors = np.linalg.eigh(A)  # –¥–ª—è —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü
        
        analysis = {
            'max_eigenvalue': np.max(eigenvalues),
            'min_eigenvalue': np.min(eigenvalues),
            'condition_number': np.max(eigenvalues) / np.min(eigenvalues),
            'trace': np.trace(A),
            'sum_eigenvalues': np.sum(eigenvalues),
            'determinant': np.linalg.det(A),
            'product_eigenvalues': np.prod(eigenvalues)
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
        dot_product = eigenvectors.T @ eigenvectors
        is_orthogonal = np.allclose(dot_product, np.eye(n))
        analysis['eigenvectors_orthogonal'] = is_orthogonal
        
        return analysis
    
    @staticmethod
    def svd_analysis():
        """–ê–Ω–∞–ª–∏–∑ —Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ–≥–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è"""
        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        m, n = 200, 150
        A = np.random.random((m, n))
        
        # SVD —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ
        U, s, Vt = np.linalg.svd(A, full_matrices=False)
        
        analysis = {
            'original_shape': A.shape,
            'U_shape': U.shape,
            'singular_values_shape': s.shape,
            'Vt_shape': Vt.shape,
            'rank': np.sum(s > 1e-10),
            'condition_number': s[0] / s[-1] if s[-1] > 1e-15 else np.inf,
            'frobenius_norm': np.linalg.norm(A, 'fro'),
            'nuclear_norm': np.sum(s)
        }
        
        # –ê–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è –Ω–∏–∑–∫–æ–≥–æ —Ä–∞–Ω–≥–∞
        ranks = [10, 30, 50, 100]
        approximations = {}
        
        for rank in ranks:
            if rank < len(s):
                A_approx = U[:, :rank] @ np.diag(s[:rank]) @ Vt[:rank, :]
                error = np.linalg.norm(A - A_approx, 'fro')
                compression_ratio = (rank * (m + n)) / (m * n)
                
                approximations[rank] = {
                    'error': error,
                    'relative_error': error / np.linalg.norm(A, 'fro'),
                    'compression_ratio': compression_ratio
                }
        
        analysis['low_rank_approximations'] = approximations
        
        return analysis

class PerformanceOptimization:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ NumPy"""
    
    @staticmethod
    def memory_layout_optimization():
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç–∏"""
        size = (1000, 1000)
        
        # C-style (row-major) vs Fortran-style (column-major)
        arr_c = np.random.random(size)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é C-style
        arr_f = np.asfortranarray(arr_c)  # Fortran-style
        
        results = {}
        
        # –¢–µ—Å—Ç –¥–æ—Å—Ç—É–ø–∞ –ø–æ —Å—Ç—Ä–æ–∫–∞–º
        start = time.time()
        for i in range(size[0]):
            _ = arr_c[i, :].sum()
        results['row_access_c_style'] = time.time() - start
        
        start = time.time()
        for i in range(size[0]):
            _ = arr_f[i, :].sum()
        results['row_access_f_style'] = time.time() - start
        
        # –¢–µ—Å—Ç –¥–æ—Å—Ç—É–ø–∞ –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º
        start = time.time()
        for j in range(size[1]):
            _ = arr_c[:, j].sum()
        results['column_access_c_style'] = time.time() - start
        
        start = time.time()
        for j in range(size[1]):
            _ = arr_f[:, j].sum()
        results['column_access_f_style'] = time.time() - start
        
        return results
    
    @staticmethod
    def cache_optimization():
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫—ç—à–∞"""
        sizes = [100, 500, 1000, 2000]
        results = {}
        
        for size in sizes:
            A = np.random.random((size, size))
            B = np.random.random((size, size))
            
            # –û–±—ã—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ
            start = time.time()
            C1 = A @ B
            time_normal = time.time() - start
            
            # –ë–ª–æ—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–∞—Ç—Ä–∏—Ü)
            block_size = min(64, size // 4)
            C2 = np.zeros((size, size))
            
            start = time.time()
            for i in range(0, size, block_size):
                for j in range(0, size, block_size):
                    for k in range(0, size, block_size):
                        i_end = min(i + block_size, size)
                        j_end = min(j + block_size, size)
                        k_end = min(k + block_size, size)
                        
                        C2[i:i_end, j:j_end] += A[i:i_end, k:k_end] @ B[k:k_end, j:j_end]
            
            time_blocked = time.time() - start
            
            results[size] = {
                'normal_time': time_normal,
                'blocked_time': time_blocked,
                'speedup': time_normal / time_blocked,
                'results_equal': np.allclose(C1, C2)
            }
        
        return results
    
    @staticmethod
    @jit(nopython=True)
    def numba_optimized_function(arr):
        """–§—É–Ω–∫—Ü–∏—è, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å –ø–æ–º–æ—â—å—é Numba"""
        result = np.zeros_like(arr)
        for i in range(arr.shape[0]):
            for j in range(arr.shape[1]):
                result[i, j] = np.sqrt(arr[i, j]**2 + 1) * np.sin(arr[i, j])
        return result
    
    @staticmethod
    def compare_numba_performance():
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å Numba"""
        size = (1000, 1000)
        arr = np.random.random(size)
        
        # –ß–∏—Å—Ç—ã–π NumPy
        start = time.time()
        result_numpy = np.sqrt(arr**2 + 1) * np.sin(arr)
        time_numpy = time.time() - start
        
        # –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ Numba (–≤–∫–ª—é—á–∞–µ—Ç –∫–æ–º–ø–∏–ª—è—Ü–∏—é)
        start = time.time()
        result_numba_first = PerformanceOptimization.numba_optimized_function(arr)
        time_numba_first = time.time() - start
        
        # –í—Ç–æ—Ä–æ–π –≤—ã–∑–æ–≤ Numba (–±–µ–∑ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏)
        start = time.time()
        result_numba_second = PerformanceOptimization.numba_optimized_function(arr)
        time_numba_second = time.time() - start
        
        return {
            'numpy_time': time_numpy,
            'numba_first_call': time_numba_first,
            'numba_second_call': time_numba_second,
            'numba_speedup': time_numpy / time_numba_second,
            'results_close': np.allclose(result_numpy, result_numba_second)
        }

# –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
class ScientificComputing:
    """–ù–∞—É—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å NumPy"""
    
    @staticmethod
    def monte_carlo_pi_estimation(n_samples: int = 1000000):
        """–û—Ü–µ–Ω–∫–∞ œÄ –º–µ—Ç–æ–¥–æ–º –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ"""
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Ç–æ—á–∫–∏ –≤ –∫–≤–∞–¥—Ä–∞—Ç–µ [-1, 1] x [-1, 1]
        points = np.random.uniform(-1, 1, (n_samples, 2))
        
        # –°—á–∏—Ç–∞–µ–º —Ç–æ—á–∫–∏ –≤–Ω—É—Ç—Ä–∏ –µ–¥–∏–Ω–∏—á–Ω–æ–≥–æ –∫—Ä—É–≥–∞
        distances_squared = np.sum(points**2, axis=1)
        inside_circle = distances_squared <= 1
        
        # –û—Ü–µ–Ω–∫–∞ œÄ
        pi_estimate = 4 * np.sum(inside_circle) / n_samples
        
        return {
            'pi_estimate': pi_estimate,
            'error': abs(pi_estimate - np.pi),
            'relative_error': abs(pi_estimate - np.pi) / np.pi * 100,
            'samples_used': n_samples
        }
    
    @staticmethod
    def numerical_integration(func: Callable, a: float, b: float, n: int = 1000000):
        """–ß–∏—Å–ª–µ–Ω–Ω–æ–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–º —Ç—Ä–∞–ø–µ—Ü–∏–π"""
        x = np.linspace(a, b, n)
        y = func(x)
        
        # –ú–µ—Ç–æ–¥ —Ç—Ä–∞–ø–µ—Ü–∏–π
        integral = np.trapz(y, x)
        
        return {
            'integral_value': integral,
            'method': 'trapezoidal',
            'intervals': n,
            'step_size': (b - a) / (n - 1)
        }
    
    @staticmethod
    def signal_processing_example():
        """–ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤"""
        # –°–æ–∑–¥–∞–µ–º —Å–æ—Å—Ç–∞–≤–Ω–æ–π —Å–∏–≥–Ω–∞–ª
        t = np.linspace(0, 1, 1000)
        signal = (np.sin(2 * np.pi * 5 * t) +  # 5 Hz
                 0.5 * np.sin(2 * np.pi * 20 * t) +  # 20 Hz
                 0.3 * np.sin(2 * np.pi * 50 * t) +  # 50 Hz
                 0.1 * np.random.normal(0, 1, len(t)))  # –®—É–º
        
        # FFT –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–ø–µ–∫—Ç—Ä–∞
        fft = np.fft.fft(signal)
        frequencies = np.fft.fftfreq(len(signal), t[1] - t[0])
        
        # –ù–∞—Ö–æ–¥–∏–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ —á–∞—Å—Ç–æ—Ç—ã
        magnitude = np.abs(fft)
        positive_freq_idx = frequencies > 0
        
        dominant_frequencies = frequencies[positive_freq_idx][
            np.argsort(magnitude[positive_freq_idx])[-5:]
        ]
        
        return {
            'signal_length': len(signal),
            'sampling_rate': 1 / (t[1] - t[0]),
            'dominant_frequencies': sorted(dominant_frequencies),
            'signal_power': np.mean(signal**2),
            'snr_estimate': np.var(signal[:-100]) / np.var(np.diff(signal))
        }

def demonstrate_numpy_capabilities():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π NumPy"""
    print("üî¢ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π NumPy")
    print("=" * 50)
    
    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
    print("\n‚ö° –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π:")
    vec_results = AdvancedNumPyOperations.vectorization_examples()
    print(f"–£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {vec_results['speedup']:.2f}x")
    for method, time_val in vec_results['times'].items():
        print(f"  {method}: {time_val:.4f} —Å–µ–∫")
    
    # –õ–∏–Ω–µ–π–Ω–∞—è –∞–ª–≥–µ–±—Ä–∞
    print("\nüßÆ –ê–Ω–∞–ª–∏–∑ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:")
    eigen_results = LinearAlgebraOperations.eigenvalue_analysis()
    print(f"–ß–∏—Å–ª–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏: {eigen_results['condition_number']:.2f}")
    print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {eigen_results['max_eigenvalue']:.4f}")
    
    # –ù–∞—É—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
    print("\nüî¨ –û—Ü–µ–Ω–∫–∞ œÄ –º–µ—Ç–æ–¥–æ–º –ú–æ–Ω—Ç–µ-–ö–∞—Ä–ª–æ:")
    pi_results = ScientificComputing.monte_carlo_pi_estimation()
    print(f"–û—Ü–µ–Ω–∫–∞ œÄ: {pi_results['pi_estimate']:.6f}")
    print(f"–û—à–∏–±–∫–∞: {pi_results['relative_error']:.4f}%")
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print("\nüöÄ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å Numba:")
    numba_results = PerformanceOptimization.compare_numba_performance()
    print(f"–£—Å–∫–æ—Ä–µ–Ω–∏–µ Numba: {numba_results['numba_speedup']:.2f}x")
    
    return {
        'vectorization': vec_results,
        'eigenanalysis': eigen_results,
        'monte_carlo': pi_results,
        'numba_performance': numba_results
    }
```

–≠—Ç–æ—Ç —Ä–∞–∑–¥–µ–ª –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∞—Å–ø–µ–∫—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–∞—É—á–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –≤ Python, –æ—Ç –±–∞–∑–æ–≤–æ–π —Ä–∞–±–æ—Ç—ã —Å Pandas –¥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ NumPy. 