"""
–£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

–≠—Ç–æ—Ç —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pandas, numpy, scikit-learn.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.datasets import make_classification, make_regression
import pytest
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import tempfile
import sqlite3
from pathlib import Path

# =============================================================================
# –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 1: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥–∞–∂ –º–∞–≥–∞–∑–∏–Ω–∞
# =============================================================================

"""
–ó–ê–î–ê–ù–ò–ï 1: Store Sales Analysis

–£ –≤–∞—Å –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥–∞–∂–∞—Ö –º–∞–≥–∞–∑–∏–Ω–∞. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ:

1. –°–æ–∑–¥–∞—Ç—å –∫–ª–∞—Å—Å SalesAnalyzer –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–¥–∞–∂
2. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã:
   - load_data() - –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
   - clean_data() - –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
   - calculate_metrics() - —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
   - find_best_products() - –ø–æ–∏—Å–∫ —Ç–æ–ø –ø—Ä–æ–¥—É–∫—Ç–æ–≤
   - analyze_trends() - –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤

3. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ –¥–∞–Ω–Ω—ã—Ö:
   - –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
   - –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–Ω—ã/–∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
   - –î—É–±–ª–∏—Ä—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏
   - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç

–î–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç: date, product_id, product_name, category, price, quantity, customer_id
"""

# –í–∞—à –∫–æ–¥ –∑–¥–µ—Å—å:
class SalesAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–¥–∞–∂"""
    
    def __init__(self):
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
        pass
    
    def load_data(self, data_source) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥–∞–∂–∞—Ö"""
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥
        pass
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û—á–∏—Å—Ç–∏—Ç—å –¥–∞–Ω–Ω—ã–µ"""
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥
        pass
    
    def calculate_metrics(self, df: pd.DataFrame) -> Dict[str, float]:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥
        pass
    
    def find_best_products(self, df: pd.DataFrame, n: int = 10) -> pd.DataFrame:
        """–ù–∞–π—Ç–∏ —Ç–æ–ø –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º"""
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥
        pass
    
    def analyze_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –ø—Ä–æ–¥–∞–∂"""
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥
        pass

# –†–µ—à–µ–Ω–∏–µ:
@dataclass
class SalesMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–¥–∞–∂"""
    total_revenue: float
    total_orders: int
    average_order_value: float
    unique_customers: int
    unique_products: int
    best_month: str
    worst_month: str

class SalesAnalyzerSolution:
    """–†–µ—à–µ–Ω–∏–µ: –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–¥–∞–∂"""
    
    def __init__(self):
        self.data = None
        self.clean_data_cache = None
    
    def generate_sample_data(self, n_records: int = 1000) -> pd.DataFrame:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        np.random.seed(42)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç
        start_date = datetime(2023, 1, 1)
        dates = [start_date + timedelta(days=x) for x in range(365)]
        
        # –ü—Ä–æ–¥—É–∫—Ç—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        products = [
            ('Laptop', 'Electronics'), ('Mouse', 'Electronics'), ('Keyboard', 'Electronics'),
            ('Monitor', 'Electronics'), ('Phone', 'Electronics'),
            ('Coffee', 'Food'), ('Tea', 'Food'), ('Cookies', 'Food'), ('Bread', 'Food'),
            ('T-Shirt', 'Clothing'), ('Jeans', 'Clothing'), ('Shoes', 'Clothing'),
            ('Book', 'Media'), ('DVD', 'Media'), ('Magazine', 'Media')
        ]
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–ø–∏—Å–µ–π
        records = []
        for _ in range(n_records):
            date = np.random.choice(dates)
            product_name, category = products[np.random.randint(0, len(products))]
            product_id = f"P{hash(product_name) % 10000:04d}"
            
            # –†–∞–∑–ª–∏—á–Ω—ã–µ —Ü–µ–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            if category == 'Electronics':
                price = np.random.uniform(50, 2000)
            elif category == 'Food':
                price = np.random.uniform(1, 50)
            elif category == 'Clothing':
                price = np.random.uniform(20, 200)
            else:  # Media
                price = np.random.uniform(5, 100)
            
            quantity = np.random.randint(1, 6)
            customer_id = f"C{np.random.randint(1, 500):04d}"
            
            records.append({
                'date': date,
                'product_id': product_id,
                'product_name': product_name,
                'category': category,
                'price': price,
                'quantity': quantity,
                'customer_id': customer_id
            })
        
        df = pd.DataFrame(records)
        
        # –î–æ–±–∞–≤–ª—è–µ–º "–ø—Ä–æ–±–ª–µ–º—ã" –≤ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –æ—á–∏—Å—Ç–∫–∏
        # –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)
        df.loc[missing_indices[:len(missing_indices)//3], 'price'] = np.nan
        df.loc[missing_indices[len(missing_indices)//3:2*len(missing_indices)//3], 'product_name'] = np.nan
        df.loc[missing_indices[2*len(missing_indices)//3:], 'customer_id'] = np.nan
        
        # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        negative_indices = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)
        df.loc[negative_indices[:len(negative_indices)//2], 'price'] = -df.loc[negative_indices[:len(negative_indices)//2], 'price']
        df.loc[negative_indices[len(negative_indices)//2:], 'quantity'] = -df.loc[negative_indices[len(negative_indices)//2:], 'quantity']
        
        # –î—É–±–ª–∏–∫–∞—Ç—ã
        duplicate_indices = np.random.choice(df.index, size=int(0.03 * len(df)), replace=False)
        duplicates = df.loc[duplicate_indices].copy()
        df = pd.concat([df, duplicates], ignore_index=True)
        
        return df
    
    def load_data(self, data_source=None) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥–∞–∂–∞—Ö"""
        if data_source is None:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            self.data = self.generate_sample_data()
        elif isinstance(data_source, str):
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞
            if data_source.endswith('.csv'):
                self.data = pd.read_csv(data_source)
            elif data_source.endswith('.xlsx'):
                self.data = pd.read_excel(data_source)
            else:
                raise ValueError("Unsupported file format")
        elif isinstance(data_source, pd.DataFrame):
            self.data = data_source.copy()
        else:
            raise ValueError("Invalid data source")
        
        return self.data
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """–û—á–∏—Å—Ç–∏—Ç—å –¥–∞–Ω–Ω—ã–µ"""
        cleaned_df = df.copy()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ –æ—á–∏—Å—Ç–∫–∏
        initial_count = len(cleaned_df)
        
        # 1. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        cleaned_df = cleaned_df.drop_duplicates()
        duplicates_removed = initial_count - len(cleaned_df)
        
        # 2. –û—á–∏—Å—Ç–∫–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        cleaned_df = cleaned_df[
            (cleaned_df['price'] > 0) & 
            (cleaned_df['quantity'] > 0)
        ].copy()
        
        # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –±–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–∞
        cleaned_df = cleaned_df.dropna(subset=['product_name'])
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Ü–µ–Ω—ã –º–µ–¥–∏–∞–Ω–æ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        for category in cleaned_df['category'].unique():
            category_median_price = cleaned_df[
                (cleaned_df['category'] == category) & 
                (cleaned_df['price'].notna())
            ]['price'].median()
            
            cleaned_df.loc[
                (cleaned_df['category'] == category) & 
                (cleaned_df['price'].isna()), 
                'price'
            ] = category_median_price
        
        # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –±–µ–∑ customer_id (–º–æ–∂–Ω–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å 'Unknown')
        cleaned_df = cleaned_df.dropna(subset=['customer_id'])
        
        # 4. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        cleaned_df['date'] = pd.to_datetime(cleaned_df['date'])
        cleaned_df['price'] = cleaned_df['price'].astype(float)
        cleaned_df['quantity'] = cleaned_df['quantity'].astype(int)
        
        # 5. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª—è–µ–º—ã—Ö –ø–æ–ª–µ–π
        cleaned_df['total_amount'] = cleaned_df['price'] * cleaned_df['quantity']
        cleaned_df['year'] = cleaned_df['date'].dt.year
        cleaned_df['month'] = cleaned_df['date'].dt.month
        cleaned_df['day_of_week'] = cleaned_df['date'].dt.dayofweek
        
        self.clean_data_cache = cleaned_df
        
        print(f"–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        print(f"  –ò—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {initial_count}")
        print(f"  –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed}")
        print(f"  –û—Å—Ç–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–µ–π: {len(cleaned_df)}")
        print(f"  –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ—Ç–µ—Ä—å: {(initial_count - len(cleaned_df)) / initial_count * 100:.1f}%")
        
        return cleaned_df
    
    def calculate_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
        metrics = {}
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics['total_revenue'] = df['total_amount'].sum()
        metrics['total_orders'] = len(df)
        metrics['average_order_value'] = df['total_amount'].mean()
        metrics['median_order_value'] = df['total_amount'].median()
        
        # –ö–ª–∏–µ–Ω—Ç—ã –∏ –ø—Ä–æ–¥—É–∫—Ç—ã
        metrics['unique_customers'] = df['customer_id'].nunique()
        metrics['unique_products'] = df['product_id'].nunique()
        metrics['unique_categories'] = df['category'].nunique()
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        monthly_sales = df.groupby(['year', 'month'])['total_amount'].sum()
        best_month_idx = monthly_sales.idxmax()
        worst_month_idx = monthly_sales.idxmin()
        
        metrics['best_month'] = f"{best_month_idx[0]}-{best_month_idx[1]:02d}"
        metrics['worst_month'] = f"{worst_month_idx[0]}-{worst_month_idx[1]:02d}"
        metrics['best_month_revenue'] = monthly_sales.max()
        metrics['worst_month_revenue'] = monthly_sales.min()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_stats = df.groupby('category').agg({
            'total_amount': ['sum', 'count', 'mean'],
            'customer_id': 'nunique'
        }).round(2)
        
        metrics['category_performance'] = category_stats
        
        # –î–µ–Ω—å –Ω–µ–¥–µ–ª–∏
        dow_sales = df.groupby('day_of_week')['total_amount'].mean()
        metrics['best_day_of_week'] = dow_sales.idxmax()  # 0=Monday, 6=Sunday
        metrics['worst_day_of_week'] = dow_sales.idxmin()
        
        return metrics
    
    def find_best_products(self, df: pd.DataFrame, n: int = 10) -> pd.DataFrame:
        """–ù–∞–π—Ç–∏ —Ç–æ–ø –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º"""
        product_performance = df.groupby(['product_id', 'product_name', 'category']).agg({
            'total_amount': ['sum', 'count', 'mean'],
            'quantity': 'sum',
            'customer_id': 'nunique'
        }).round(2)
        
        # –£–ø—Ä–æ—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
        product_performance.columns = [
            'total_revenue', 'order_count', 'avg_order_value', 
            'total_quantity', 'unique_customers'
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        product_performance['revenue_per_customer'] = (
            product_performance['total_revenue'] / product_performance['unique_customers']
        ).round(2)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—ã—Ä—É—á–∫–µ
        top_products = product_performance.sort_values('total_revenue', ascending=False).head(n)
        
        return top_products.reset_index()
    
    def analyze_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –ø—Ä–æ–¥–∞–∂"""
        trends = {}
        
        # –ï–∂–µ–º–µ—Å—è—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã
        monthly_trends = df.groupby(['year', 'month']).agg({
            'total_amount': 'sum',
            'quantity': 'sum',
            'customer_id': 'nunique'
        }).reset_index()
        
        monthly_trends['date'] = pd.to_datetime(monthly_trends[['year', 'month']].assign(day=1))
        monthly_trends = monthly_trends.sort_values('date')
        
        # –†–æ—Å—Ç –ø–æ –º–µ—Å—è—Ü–∞–º
        monthly_trends['revenue_growth'] = monthly_trends['total_amount'].pct_change() * 100
        monthly_trends['customer_growth'] = monthly_trends['customer_id'].pct_change() * 100
        
        trends['monthly_data'] = monthly_trends
        trends['avg_monthly_growth'] = monthly_trends['revenue_growth'].mean()
        
        # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (–ø–æ –º–µ—Å—è—Ü–∞–º)
        seasonal_pattern = df.groupby('month')['total_amount'].mean()
        trends['seasonal_pattern'] = seasonal_pattern
        trends['peak_season_month'] = seasonal_pattern.idxmax()
        trends['low_season_month'] = seasonal_pattern.idxmin()
        
        # –¢—Ä–µ–Ω–¥—ã –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
        dow_pattern = df.groupby('day_of_week')['total_amount'].mean()
        trends['daily_pattern'] = dow_pattern
        
        # –ö–æ–≥–æ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
        # –ü–µ—Ä–≤–∞—è –ø–æ–∫—É–ø–∫–∞ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
        customer_first_purchase = df.groupby('customer_id')['date'].min().reset_index()
        customer_first_purchase['cohort_month'] = customer_first_purchase['date'].dt.to_period('M')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–≥–æ—Ä—Ç—É –∫ –æ—Å–Ω–æ–≤–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        df_with_cohort = df.merge(
            customer_first_purchase[['customer_id', 'cohort_month']], 
            on='customer_id'
        )
        df_with_cohort['order_month'] = df_with_cohort['date'].dt.to_period('M')
        
        # –ü–µ—Ä–∏–æ–¥ —Å –º–æ–º–µ–Ω—Ç–∞ –ø–µ—Ä–≤–æ–π –ø–æ–∫—É–ø–∫–∏
        df_with_cohort['period_number'] = (
            df_with_cohort['order_month'] - df_with_cohort['cohort_month']
        ).apply(attrgetter('n'))
        
        # –¢–∞–±–ª–∏—Ü–∞ –∫–æ–≥–æ—Ä—Ç (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        cohort_table = df_with_cohort.groupby(['cohort_month', 'period_number'])['customer_id'].nunique().reset_index()
        cohort_sizes = customer_first_purchase.groupby('cohort_month')['customer_id'].nunique()
        
        trends['cohort_analysis'] = {
            'cohort_table': cohort_table,
            'cohort_sizes': cohort_sizes
        }
        
        return trends

from operator import attrgetter

# –¢–µ—Å—Ç—ã –¥–ª—è SalesAnalyzer
class TestSalesAnalyzer:
    """–¢–µ—Å—Ç—ã –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –ø—Ä–æ–¥–∞–∂"""
    
    @pytest.fixture
    def analyzer(self):
        """Fixture –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –ø—Ä–æ–¥–∞–∂"""
        return SalesAnalyzerSolution()
    
    @pytest.fixture
    def sample_data(self, analyzer):
        """Fixture —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        return analyzer.generate_sample_data(100)
    
    def test_load_data(self, analyzer):
        """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        data = analyzer.load_data()
        
        assert isinstance(data, pd.DataFrame)
        assert len(data) > 0
        assert all(col in data.columns for col in ['date', 'product_id', 'price', 'quantity'])
    
    def test_clean_data(self, analyzer, sample_data):
        """–¢–µ—Å—Ç –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        cleaned = analyzer.clean_data(sample_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        assert (cleaned['price'] > 0).all()
        assert (cleaned['quantity'] > 0).all()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤—ã—á–∏—Å–ª—è–µ–º—ã–µ –ø–æ–ª—è
        assert 'total_amount' in cleaned.columns
        assert 'year' in cleaned.columns
        assert 'month' in cleaned.columns
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ total_amount —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ
        assert (cleaned['total_amount'] == cleaned['price'] * cleaned['quantity']).all()
    
    def test_calculate_metrics(self, analyzer, sample_data):
        """–¢–µ—Å—Ç —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫"""
        cleaned_data = analyzer.clean_data(sample_data)
        metrics = analyzer.calculate_metrics(cleaned_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        assert 'total_revenue' in metrics
        assert 'total_orders' in metrics
        assert 'average_order_value' in metrics
        assert 'unique_customers' in metrics
        assert 'unique_products' in metrics
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–≥–∏–∫—É —Ä–∞—Å—á–µ—Ç–æ–≤
        expected_total_revenue = cleaned_data['total_amount'].sum()
        assert abs(metrics['total_revenue'] - expected_total_revenue) < 0.01
        
        assert metrics['total_orders'] == len(cleaned_data)
    
    def test_find_best_products(self, analyzer, sample_data):
        """–¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ —Ç–æ–ø –ø—Ä–æ–¥—É–∫—Ç–æ–≤"""
        cleaned_data = analyzer.clean_data(sample_data)
        top_products = analyzer.find_best_products(cleaned_data, n=5)
        
        assert len(top_products) <= 5
        assert 'total_revenue' in top_products.columns
        assert 'product_name' in top_products.columns
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤—ã—Ä—É—á–∫–∏
        revenues = top_products['total_revenue'].values
        assert all(revenues[i] >= revenues[i+1] for i in range(len(revenues)-1))

# =============================================================================
# –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 2: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å
# =============================================================================

"""
–ó–ê–î–ê–ù–ò–ï 2: House Price Prediction

–°–æ–∑–¥–∞–π—Ç–µ —Å–∏—Å—Ç–µ–º—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å:

1. –ö–ª–∞—Å—Å HousePricePredictor
2. –ú–µ—Ç–æ–¥—ã:
   - prepare_features() - –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
   - train_model() - –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
   - evaluate_model() - –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
   - predict_price() - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω—ã
   - feature_importance() - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

3. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö:
   - –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø–ª–æ—â–∞–¥—å, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç, –≤–æ–∑—Ä–∞—Å—Ç)
   - –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ä–∞–π–æ–Ω, —Ç–∏–ø –¥–æ–º–∞)
   - Feature engineering (—Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –ª—É—á—à–∏–π.
"""

# –í–∞—à –∫–æ–¥ –∑–¥–µ—Å—å:
class HousePricePredictor:
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å —Ü–µ–Ω –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å"""
    
    def __init__(self):
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
        pass
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥
        pass
    
    def train_model(self, X: pd.DataFrame, y: pd.Series):
        """–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å"""
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥
        pass
    
    def evaluate_model(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏"""
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥
        pass
    
    def predict_price(self, house_features: Dict[str, Any]) -> float:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ü–µ–Ω—É –¥–æ–º–∞"""
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥
        pass

# –†–µ—à–µ–Ω–∏–µ:
class HousePricePredictorSolution:
    """–†–µ—à–µ–Ω–∏–µ: –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å —Ü–µ–Ω –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å"""
    
    def __init__(self):
        self.models = {}
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.feature_columns = None
        self.best_model_name = None
        self.training_data = None
    
    def generate_house_data(self, n_houses: int = 1000) -> pd.DataFrame:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
        np.random.seed(42)
        
        # –ë–∞–∑–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        areas = np.random.normal(150, 50, n_houses)  # –ø–ª–æ—â–∞–¥—å –≤ –∫–≤.–º
        areas = np.clip(areas, 50, 500)  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º–∏ –ø—Ä–µ–¥–µ–ª–∞–º–∏
        
        rooms = np.random.poisson(3, n_houses) + 1  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç
        rooms = np.clip(rooms, 1, 8)
        
        age = np.random.exponential(15, n_houses)  # –≤–æ–∑—Ä–∞—Å—Ç –¥–æ–º–∞
        age = np.clip(age, 0, 100)
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        districts = np.random.choice(['Downtown', 'Suburb', 'Outskirts'], n_houses, p=[0.3, 0.5, 0.2])
        house_types = np.random.choice(['Apartment', 'House', 'Townhouse'], n_houses, p=[0.6, 0.3, 0.1])
        conditions = np.random.choice(['Excellent', 'Good', 'Fair', 'Poor'], n_houses, p=[0.2, 0.4, 0.3, 0.1])
        
        # Feature engineering
        area_per_room = areas / rooms
        is_new = (age < 5).astype(int)
        is_large = (areas > 200).astype(int)
        
        # –†–∞—Å—á–µ—Ç —Ü–µ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º —à—É–º–æ–º)
        price_base = (
            areas * 2000 +  # –±–∞–∑–æ–≤–∞—è —Ü–µ–Ω–∞ –∑–∞ –∫–≤.–º
            rooms * 10000 +  # –¥–æ–ø–ª–∞—Ç–∞ –∑–∞ –∫–æ–º–Ω–∞—Ç—ã
            -age * 500 +  # —Å–∫–∏–¥–∫–∞ –∑–∞ –≤–æ–∑—Ä–∞—Å—Ç
            area_per_room * 1000  # –¥–æ–ø–ª–∞—Ç–∞ –∑–∞ –ø—Ä–æ—Å—Ç–æ—Ä–Ω–æ—Å—Ç—å
        )
        
        # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–æ —Ä–∞–π–æ–Ω—É
        district_multiplier = {'Downtown': 1.5, 'Suburb': 1.0, 'Outskirts': 0.7}
        price_base *= [district_multiplier[d] for d in districts]
        
        # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–æ —Ç–∏–ø—É –¥–æ–º–∞
        type_multiplier = {'House': 1.2, 'Townhouse': 1.0, 'Apartment': 0.9}
        price_base *= [type_multiplier[t] for t in house_types]
        
        # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–æ —Å–æ—Å—Ç–æ—è–Ω–∏—é
        condition_multiplier = {'Excellent': 1.2, 'Good': 1.0, 'Fair': 0.8, 'Poor': 0.6}
        price_base *= [condition_multiplier[c] for c in conditions]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º
        noise = np.random.normal(0, 20000, n_houses)
        prices = price_base + noise
        prices = np.clip(prices, 50000, 2000000)  # —Ä–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã —Ü–µ–Ω
        
        return pd.DataFrame({
            'area': areas,
            'rooms': rooms,
            'age': age,
            'district': districts,
            'house_type': house_types,
            'condition': conditions,
            'area_per_room': area_per_room,
            'is_new': is_new,
            'is_large': is_large,
            'price': prices
        })
    
    def prepare_features(self, df: pd.DataFrame, is_training: bool = True) -> Tuple[pd.DataFrame, Optional[pd.Series]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        feature_df = df.copy()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if 'area' in feature_df.columns and 'rooms' in feature_df.columns:
            feature_df['area_per_room'] = feature_df['area'] / feature_df['rooms']
        
        if 'age' in feature_df.columns:
            feature_df['is_new'] = (feature_df['age'] < 5).astype(int)
            feature_df['age_category'] = pd.cut(feature_df['age'], 
                                               bins=[0, 5, 15, 30, 100], 
                                               labels=['New', 'Recent', 'Mature', 'Old'])
        
        if 'area' in feature_df.columns:
            feature_df['is_large'] = (feature_df['area'] > 200).astype(int)
            feature_df['area_category'] = pd.cut(feature_df['area'], 
                                                bins=[0, 100, 150, 200, 500], 
                                                labels=['Small', 'Medium', 'Large', 'XLarge'])
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        categorical_columns = ['district', 'house_type', 'condition', 'age_category', 'area_category']
        
        for col in categorical_columns:
            if col in feature_df.columns:
                if is_training:
                    # –°–æ–∑–¥–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º encoder
                    self.label_encoders[col] = LabelEncoder()
                    feature_df[col] = self.label_encoders[col].fit_transform(feature_df[col].astype(str))
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π encoder
                    if col in self.label_encoders:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                        unique_values = set(self.label_encoders[col].classes_)
                        feature_df[col] = feature_df[col].astype(str)
                        
                        # –ó–∞–º–µ–Ω—è–µ–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ —Å–∞–º—É—é —á–∞—Å—Ç—É—é
                        most_frequent = self.label_encoders[col].classes_[0]
                        feature_df[col] = feature_df[col].apply(
                            lambda x: x if x in unique_values else most_frequent
                        )
                        
                        feature_df[col] = self.label_encoders[col].transform(feature_df[col])
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∏—Å–∫–ª—é—á–∞–µ–º —Ü–µ–Ω—É)
        feature_columns = [col for col in feature_df.columns if col != 'price']
        X = feature_df[feature_columns]
        
        if is_training:
            self.feature_columns = feature_columns
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if is_training:
            X_scaled = self.scaler.fit_transform(X)
        else:
            X_scaled = self.scaler.transform(X)
        
        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_columns, index=X.index)
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        y = df['price'] if 'price' in df.columns else None
        
        return X_scaled_df, y
    
    def train_model(self, df: pd.DataFrame):
        """–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏"""
        self.training_data = df.copy()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y = self.prepare_features(df, is_training=True)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # –†–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏
        models_to_train = {
            'Linear Regression': LinearRegression(),
            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'Random Forest Tuned': RandomForestRegressor(
                n_estimators=200, 
                max_depth=15, 
                min_samples_split=5,
                random_state=42
            )
        }
        
        best_score = float('-inf')
        
        print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:")
        for name, model in models_to_train.items():
            print(f"\n--- {name} ---")
            
            # –û–±—É—á–µ–Ω–∏–µ
            model.fit(X_train, y_train)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred_train = model.predict(X_train)
            y_pred_test = model.predict(X_test)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            train_r2 = r2_score(y_train, y_pred_train)
            test_r2 = r2_score(y_test, y_pred_test)
            train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
            test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
            
            print(f"Train R¬≤: {train_r2:.3f}, RMSE: {train_rmse:.0f}")
            print(f"Test R¬≤: {test_r2:.3f}, RMSE: {test_rmse:.0f}")
            
            # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
            print(f"CV R¬≤ (5-fold): {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.models[name] = {
                'model': model,
                'train_r2': train_r2,
                'test_r2': test_r2,
                'train_rmse': train_rmse,
                'test_rmse': test_rmse,
                'cv_r2_mean': cv_scores.mean(),
                'cv_r2_std': cv_scores.std()
            }
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            if test_r2 > best_score:
                best_score = test_r2
                self.best_model_name = name
        
        print(f"\n–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {self.best_model_name} (Test R¬≤: {best_score:.3f})")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.X_test = X_test
        self.y_test = y_test
        
        return self.models
    
    def evaluate_model(self, model_name: str = None) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏"""
        if model_name is None:
            model_name = self.best_model_name
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not found")
        
        model_info = self.models[model_name]
        
        evaluation = {
            'model_name': model_name,
            'train_r2': model_info['train_r2'],
            'test_r2': model_info['test_r2'],
            'train_rmse': model_info['train_rmse'],
            'test_rmse': model_info['test_rmse'],
            'cv_r2_mean': model_info['cv_r2_mean'],
            'cv_r2_std': model_info['cv_r2_std']
        }
        
        return evaluation
    
    def feature_importance(self, model_name: str = None) -> pd.DataFrame:
        """–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if model_name is None:
            model_name = self.best_model_name
        
        if model_name not in self.models:
            raise ValueError(f"Model {model_name} not found")
        
        model = self.models[model_name]['model']
        
        if hasattr(model, 'feature_importances_'):
            importance_df = pd.DataFrame({
                'feature': self.feature_columns,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            return importance_df
        else:
            # –î–ª—è –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã
            if hasattr(model, 'coef_'):
                importance_df = pd.DataFrame({
                    'feature': self.feature_columns,
                    'coefficient': model.coef_,
                    'abs_coefficient': np.abs(model.coef_)
                }).sort_values('abs_coefficient', ascending=False)
                
                return importance_df
        
        return pd.DataFrame()
    
    def predict_price(self, house_features: Dict[str, Any]) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ü–µ–Ω—É –¥–æ–º–∞"""
        if not self.models or self.best_model_name is None:
            raise ValueError("Model not trained yet")
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        input_df = pd.DataFrame([house_features])
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        X_input, _ = self.prepare_features(input_df, is_training=False)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª—å—é
        best_model = self.models[self.best_model_name]['model']
        prediction = best_model.predict(X_input)[0]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤—Å–µ–º–∏ –º–æ–¥–µ–ª—è–º–∏
        all_predictions = {}
        for name, model_info in self.models.items():
            model_pred = model_info['model'].predict(X_input)[0]
            all_predictions[name] = model_pred
        
        return {
            'predicted_price': prediction,
            'model_used': self.best_model_name,
            'all_predictions': all_predictions,
            'input_features': house_features
        }

# –¢–µ—Å—Ç—ã –¥–ª—è HousePricePredictor
class TestHousePricePredictor:
    """–¢–µ—Å—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—è —Ü–µ–Ω –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å"""
    
    @pytest.fixture
    def predictor(self):
        """Fixture –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—è"""
        return HousePricePredictorSolution()
    
    @pytest.fixture
    def house_data(self, predictor):
        """Fixture —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"""
        return predictor.generate_house_data(200)
    
    def test_generate_house_data(self, predictor):
        """–¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
        data = predictor.generate_house_data(100)
        
        assert isinstance(data, pd.DataFrame)
        assert len(data) == 100
        assert 'price' in data.columns
        assert 'area' in data.columns
        assert 'rooms' in data.columns
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        assert (data['price'] > 0).all()
        assert (data['area'] > 0).all()
        assert (data['rooms'] > 0).all()
    
    def test_prepare_features(self, predictor, house_data):
        """–¢–µ—Å—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        X, y = predictor.prepare_features(house_data, is_training=True)
        
        assert isinstance(X, pd.DataFrame)
        assert isinstance(y, pd.Series)
        assert len(X) == len(y)
        assert 'price' not in X.columns  # —Ü–µ–Ω–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    
    def test_train_model(self, predictor, house_data):
        """–¢–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
        models = predictor.train_model(house_data)
        
        assert len(models) > 0
        assert predictor.best_model_name is not None
        assert predictor.best_model_name in models
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        best_model_info = models[predictor.best_model_name]
        assert best_model_info['test_r2'] > 0.5  # –†–∞–∑—É–º–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    
    def test_predict_price(self, predictor, house_data):
        """–¢–µ—Å—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–Ω—ã"""
        predictor.train_model(house_data)
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–æ–º–∞
        test_house = {
            'area': 150,
            'rooms': 3,
            'age': 10,
            'district': 'Suburb',
            'house_type': 'House',
            'condition': 'Good'
        }
        
        result = predictor.predict_price(test_house)
        
        assert 'predicted_price' in result
        assert 'model_used' in result
        assert result['predicted_price'] > 0
        assert isinstance(result['predicted_price'], (int, float))

# =============================================================================
# –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 3: –ê–Ω–∞–ª–∏–∑ –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
# =============================================================================

"""
–ó–ê–î–ê–ù–ò–ï 3: Customer Segmentation

–°–æ–∑–¥–∞–π—Ç–µ —Å–∏—Å—Ç–µ–º—É —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤:

1. –ö–ª–∞—Å—Å CustomerSegmentation
2. RFM –∞–Ω–∞–ª–∏–∑ (Recency, Frequency, Monetary)
3. K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
4. –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
5. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–±–æ—Ç–µ —Å —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏

–î–∞–Ω–Ω—ã–µ: customer_id, purchase_date, amount, product_category
"""

# –í–∞—à –∫–æ–¥ –∑–¥–µ—Å—å:
class CustomerSegmentation:
    """–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
        pass
    
    def calculate_rfm(self, df: pd.DataFrame) -> pd.DataFrame:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å RFM –º–µ—Ç—Ä–∏–∫–∏"""
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥
        pass
    
    def perform_clustering(self, rfm_df: pd.DataFrame, n_clusters: int = 4):
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é"""
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥
        pass
    
    def profile_segments(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ã"""
        # TODO: –†–µ–∞–ª–∏–∑—É–π—Ç–µ –º–µ—Ç–æ–¥
        pass

# –†–µ—à–µ–Ω–∏–µ (–∫—Ä–∞—Ç–∫–∞—è –≤–µ—Ä—Å–∏—è):
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

class CustomerSegmentationSolution:
    """–†–µ—à–µ–Ω–∏–µ: –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.kmeans = None
        self.scaler = StandardScaler()
        self.rfm_data = None
        self.segments = None
    
    def generate_customer_data(self, n_customers: int = 500, n_transactions: int = 5000) -> pd.DataFrame:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –∫–ª–∏–µ–Ω—Ç–∞—Ö"""
        np.random.seed(42)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
        transactions = []
        customer_ids = [f"C{i:04d}" for i in range(1, n_customers + 1)]
        
        start_date = datetime(2023, 1, 1)
        end_date = datetime(2023, 12, 31)
        
        categories = ['Electronics', 'Clothing', 'Food', 'Books', 'Home']
        
        for _ in range(n_transactions):
            customer_id = np.random.choice(customer_ids)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é –¥–∞—Ç—É
            days_diff = (end_date - start_date).days
            random_days = np.random.randint(0, days_diff)
            purchase_date = start_date + timedelta(days=random_days)
            
            # –°—É–º–º–∞ –ø–æ–∫—É–ø–∫–∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            category = np.random.choice(categories)
            if category == 'Electronics':
                amount = np.random.uniform(100, 2000)
            elif category == 'Clothing':
                amount = np.random.uniform(20, 500)
            elif category == 'Food':
                amount = np.random.uniform(5, 100)
            elif category == 'Books':
                amount = np.random.uniform(10, 80)
            else:  # Home
                amount = np.random.uniform(30, 800)
            
            transactions.append({
                'customer_id': customer_id,
                'purchase_date': purchase_date,
                'amount': amount,
                'product_category': category
            })
        
        return pd.DataFrame(transactions)
    
    def calculate_rfm(self, df: pd.DataFrame, reference_date: datetime = None) -> pd.DataFrame:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å RFM –º–µ—Ç—Ä–∏–∫–∏"""
        if reference_date is None:
            reference_date = df['purchase_date'].max() + timedelta(days=1)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º
        rfm = df.groupby('customer_id').agg({
            'purchase_date': lambda x: (reference_date - x.max()).days,  # Recency
            'amount': ['count', 'sum']  # Frequency, Monetary
        })
        
        # –£–ø—Ä–æ—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
        rfm.columns = ['Recency', 'Frequency', 'Monetary']
        
        # RFM scores (1-5, –≥–¥–µ 5 - –ª—É—á—à–µ)
        rfm['R_Score'] = pd.qcut(rfm['Recency'], 5, labels=[5,4,3,2,1], duplicates='drop')
        rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5], duplicates='drop')
        rfm['M_Score'] = pd.qcut(rfm['Monetary'], 5, labels=[1,2,3,4,5], duplicates='drop')
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RFM score
        rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)
        
        self.rfm_data = rfm
        return rfm
    
    def perform_clustering(self, rfm_df: pd.DataFrame = None, n_clusters: int = 4):
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é"""
        if rfm_df is None:
            rfm_df = self.rfm_data
        
        if rfm_df is None:
            raise ValueError("RFM data not calculated. Run calculate_rfm first.")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        clustering_data = rfm_df[['Recency', 'Frequency', 'Monetary']]
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        scaled_data = self.scaler.fit_transform(clustering_data)
        
        # K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = self.kmeans.fit_predict(scaled_data)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –∫ RFM –¥–∞–Ω–Ω—ã–º
        rfm_df['Cluster'] = clusters
        
        self.segments = rfm_df
        return rfm_df
    
    def profile_segments(self, df: pd.DataFrame = None) -> Dict[str, Any]:
        """–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ã"""
        if df is None:
            df = self.segments
        
        if df is None or 'Cluster' not in df.columns:
            raise ValueError("Clustering not performed. Run perform_clustering first.")
        
        profiles = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        cluster_stats = df.groupby('Cluster').agg({
            'Recency': ['mean', 'median'],
            'Frequency': ['mean', 'median'],
            'Monetary': ['mean', 'median'],
            'R_Score': 'mean',
            'F_Score': 'mean',
            'M_Score': 'mean'
        }).round(2)
        
        profiles['cluster_statistics'] = cluster_stats
        
        # –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_sizes = df['Cluster'].value_counts().sort_index()
        profiles['cluster_sizes'] = cluster_sizes
        
        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        interpretations = {}
        for cluster in df['Cluster'].unique():
            cluster_data = df[df['Cluster'] == cluster]
            
            avg_recency = cluster_data['Recency'].mean()
            avg_frequency = cluster_data['Frequency'].mean()
            avg_monetary = cluster_data['Monetary'].mean()
            
            # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
            if avg_recency < 30 and avg_frequency > 5 and avg_monetary > 500:
                interpretation = "Champions (–õ—É—á—à–∏–µ –∫–ª–∏–µ–Ω—Ç—ã)"
            elif avg_recency < 60 and avg_frequency > 3:
                interpretation = "Loyal Customers (–õ–æ—è–ª—å–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã)"
            elif avg_monetary > 1000:
                interpretation = "Big Spenders (–ú–Ω–æ–≥–æ —Ç—Ä–∞—Ç—è—â–∏–µ)"
            elif avg_recency > 180:
                interpretation = "Lost Customers (–ü–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã)"
            else:
                interpretation = "Regular Customers (–û–±—ã—á–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã)"
            
            interpretations[cluster] = interpretation
        
        profiles['interpretations'] = interpretations
        
        return profiles

# =============================================================================
# –ó–∞–ø—É—Å–∫ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π
# =============================================================================

def run_exercises():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π"""
    print("=== –£–ø—Ä–∞–∂–Ω–µ–Ω–∏—è: –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ ML ===\n")
    
    # 1. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥–∞–∂
    print("1. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥–∞–∂ –º–∞–≥–∞–∑–∏–Ω–∞...")
    analyzer = SalesAnalyzerSolution()
    sales_data = analyzer.load_data()
    cleaned_data = analyzer.clean_data(sales_data)
    metrics = analyzer.calculate_metrics(cleaned_data)
    
    print(f"   –û–±—â–∞—è –≤—ã—Ä—É—á–∫–∞: ${metrics['total_revenue']:,.0f}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–∞–∑–æ–≤: {metrics['total_orders']}")
    print(f"   –°—Ä–µ–¥–Ω–∏–π —á–µ–∫: ${metrics['average_order_value']:.0f}")
    
    # 2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å
    print("\n2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ü–µ–Ω –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å...")
    predictor = HousePricePredictorSolution()
    house_data = predictor.generate_house_data(500)
    models = predictor.train_model(house_data)
    
    # –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    test_house = {
        'area': 150,
        'rooms': 3,
        'age': 10,
        'district': 'Suburb',
        'house_type': 'House',
        'condition': 'Good'
    }
    
    prediction = predictor.predict_price(test_house)
    print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞ –¥–æ–º–∞: ${prediction['predicted_price']:,.0f}")
    print(f"   –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {prediction['model_used']}")
    
    # 3. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
    print("\n3. –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤...")
    segmentation = CustomerSegmentationSolution()
    customer_data = segmentation.generate_customer_data(200, 2000)
    
    rfm = segmentation.calculate_rfm(customer_data)
    clustered_rfm = segmentation.perform_clustering(rfm, n_clusters=4)
    profiles = segmentation.profile_segments()
    
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: {len(rfm)}")
    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(profiles['cluster_sizes'])}")
    
    for cluster, interpretation in profiles['interpretations'].items():
        size = profiles['cluster_sizes'][cluster]
        print(f"   –°–µ–≥–º–µ–Ω—Ç {cluster}: {interpretation} ({size} –∫–ª–∏–µ–Ω—Ç–æ–≤)")
    
    print("\n‚úÖ –í—Å–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
    print("üìä –¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏ —Å—Ç—Ä–æ–∏—Ç—å ML –º–æ–¥–µ–ª–∏!")

if __name__ == "__main__":
    run_exercises() 